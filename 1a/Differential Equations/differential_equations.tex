\documentclass{article}

\def\npart {IA}
\def\nterm {Michaelmas}
\def\nyear {2023}
\def\nlecturer {Prof A.\ D.\ Challinor}
\def\ncourse {Differential Equations}

\setcounter{tocdepth}{3}
\input{../../header.tex}

\begin{document}
\maketitle{
    \small
    \noindent\textbf{Basic calculus}\\
    Informal treatment of differentiation as a limit, the chain rule, Leibnitz's rule, Taylor series, informal treatment of $O$ and $o$ notation and l'H\^opital's rule; integration as an area, fundamental theorem of calculus, integration by substitution and parts.\hspace*{\fill}[3]
  
    \vspace{5pt}
    \noindent Informal treatment of partial derivatives, geometrical interpretation, statement (only) of symmetry of mixed partial derivatives, chain rule, implicit differentiation. Informal treatment of differentials, including exact differentials. Differentiation of an integral with respect to a parameter.\hspace*{\fill}[2]
  
    \vspace{10pt}
    \noindent\textbf{First-order linear differential equations}\\
    Equations with constant coefficients: exponential growth, comparison with discrete equations, series solution; modelling examples including radioactive decay.
  
    \vspace{5pt}
    \noindent Equations with non-constant coefficients: solution by integrating factor.\hspace*{\fill}[2]
  
    \vspace{10pt}
    \noindent\textbf{Nonlinear first-order equations}\\
    Separable equations. Exact equations. Sketching solution trajectories. Equilibrium solutions, stability by perturbation; examples, including logistic equation and chemical kinetics. Discrete equations: equilibrium solutions, stability; examples including the logistic map.\hspace*{\fill}[4]
  
    \vspace{10pt}
    \noindent\textbf{Higher-order linear differential equations}\\
    Complementary function and particular integral, linear independence, Wronskian (for second-order equations), Abel's theorem. Equations with constant coefficients and examples including radioactive sequences, comparison in simple cases with difference equations, reduction of order, resonance, transients, damping. Homogeneous equations. Response to step and impulse function inputs; introduction to the notions of the Heaviside step-function and the Dirac delta-function. Series solutions including statement only of the need for the logarithmic solution.\hspace*{\fill}[8]
  
    \vspace{10pt}
    \noindent\textbf{Multivariate functions: applications}\\
    Directional derivatives and the gradient vector. Statement of Taylor series for functions on $\R^n$. Local extrema of real functions, classification using the Hessian matrix. Coupled first order systems: equivalence to single higher order equations; solution by matrix methods. Non-degenerate phase portraits local to equilibrium points; stability.
    
    \vspace{5pt}
    \noindent Simple examples of first- and second-order partial differential equations, solution of the wave equation in the form $f(x + ct) + g(x - ct)$.\hspace*{\fill}[5]}

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%
% Lecture 1: 6/10/2023
%%%%%%%%%%%%%%%%%%%%%%

\section{Basic Calculus}
\subsection{Differentiation}
\begin{defi}[Derivative of a function]
    Define the derivative of $f(x)$ w.r.t its argument $x$ as the function
    \[
        \diff{f}{x} \equiv \lim_{h \rightarrow 0}{\frac{f(x +h) - f(x)}{h}}
    \]

    % Insert the diagram of the slopy thing.

    Note that for a derivative to exist at point $x$, both the left and right hand limits must exist and be equal. 
    \[
        \lim_{h \rightarrow 0^-}{\frac{f(x +h) - f(x)}{h}} = \lim_{h \rightarrow 0^+}{\frac{f(x +h) - f(x)}{h}}
    \]
\end{defi}

\begin{eg}[$f(x) = \abs{x}$]
    \begin{align*}
        \text{LHL} &= \lim_{h \rightarrow 0^-}{\frac{\abs{h}}{h}} = -1 \\
        \text{RHL} &= \lim_{h \rightarrow 0^-}{\frac{\abs{h}}{h}} = 1
    \end{align*}
    We see that $\abs{x}$ is not differentiable at $x=0$
\end{eg}

Some notational differences. Most often Newton's notation is used to denote differentiation with respect to time. 
\begin{center}
    \begin{tabular}{ c|c|c }    
        Leibnitz & Lagrange & Newton \\
        \midrule
        $\diff{f}{x}$ & $f^{\prime}(x)$ & $\dot{f}(x)$
    \end{tabular}
\end{center}

For sufficiently smooth functions, we can define higher derivatives. Here are some second derivatives
\[
    \diff{}{x}\left(\diff{f}{x}\right) = \diff[2]{f}{x} \quad \text{or} \quad f^{\prime\prime}(x) \quad \text{or} \quad \ddot{f}(x)
\]
For $n^{th}$ derivatives we tend to use this notation
\[
    f^{(n)}(x)
\]

\subsubsection{Big $O$ and little $\underline{o}$ notation}.
These are known as order parameters and are very useful when comparing the behaviour of 2 functions sufficiently close to some point $x_o$ or as $x$ tends to infinity. They are particular useful in deal with approximations where the remainder depends upon a parameter.

\begin{defi}[$O$ and $\underline{o}$]\leavevmode
    \begin{enumerate}
        \item $f(x)$ is $O(g(x))$ as $x \rightarrow x_0$ if $\exists \delta > 0$ and $M > 0$ such that $\forall x$ with $0 < \abs{x - x_0} < \delta$,
        \[
            \abs{f(x)} \leq M\abs{g(x)}  
        \]
        It follows from this that $\frac{f(x)}{g(x)}$ remains bounded as $x \rightarrow x_0$
        \item $f(x)$ is $\underline{o}(g(x))$ as $x \rightarrow x_0$ if $\forall \epsilon > 0, \exists \delta > 0$ such that $\forall x$ with $0 < \abs{x - x_0} < \delta$,
        \[
            \abs{f(x)} \leq \epsilon\abs{g(x)}  
        \]
        This means that $f(x)$ needs to be much smaller than $g(x)$. It follows similarly that if $g(x) \neq 0 $
        \[
            \lim_{x \rightarrow x_0}{\frac{f(x)}{g(x)}} = 0  
        \]
    \end{enumerate}
    We often write that $f(x) = O(g(x))$ or $f(x) = \underline{o}(g(x))$, but this is an abuse of notation. A more accurate notation would be to write
    \[
        f(x) \in O(g(x)) \quad f(x) \in \underline{o}(g(x))
    \]
    Since these actually represent classes of functions. We can also notice that $\underline{o}$ is a much stronger statement than $O$ since, for $\underline{o}$ we need to be able to produce a value of $x$ that results in an arbitrarily small multiple of $g(x)$

    We can also extend this to behaviour as $x \rightarrow \infty$. We say that $f(x) = O(g(x))$ if $\exists X > 0$ and $M > 0$ such that $\forall x > X$,
    \[
        \abs{f(x)} \leq M\abs{g(x)}  
    \]

    Note that
    \[
        f(x) = \underline{0}(g(x)) \Rightarrow f(x) = O(g(x))  
    \]
\end{defi}

\begin{eg}\leavevmode
    \begin{enumerate}
        \item Given $f(x) = 2x, \quad f(x) = O(x)$ as $x \rightarrow 0$. However, $f(x) \neq \underline{o}(g(x))$ as $x \rightarrow 0$ since,
        \[
            \lim_{x \rightarrow 0}{\abs{\frac{f(x)}{x}}} = 2 \neq 0  
        \]
        \item Given $f(x) = x^2, \quad f(x) = \underline{o}(x)$ as $x \rightarrow 0$ since, 
        \[
            \lim_{x \rightarrow 0}{\abs{\frac{x^2}{x}}} = 0
        \]
        \item Given $f(x) = x^2 + x, \quad f(x) = O(x^2)$ as $x \rightarrow \infty$ since for any $x > 1$,
        \[
            \abs{x^2 + x} \leq 2\abs{x^2}  
        \]
        In general, any polynomial where $a_n \neq 0$
        \[
            a_nx^n + a_{n-1}x^{n-1} + \cdots + a_1x + a_0 = O(x^n) \quad \text{as} \quad x \rightarrow 0
        \]
        \item Given $f(x) = \sin{2x}, \quad f(x) = O(x)$ as $x \rightarrow 0$ since, $\sin{2x} \approx 2x$ for $x << 1$
    \end{enumerate}
\end{eg}

\begin{remark}
    When using order paramters, constants do not matter. If 
    \begin{align*}
        f(x) &= O(g(x)) \Rightarrow af(x) = O(g(x)) \\
        f(x) &= O(ag(x)) \quad \text{for} \ a \neq 0
    \end{align*}
\end{remark}

Order paramters are useful to classify remainder terms before taking a limit. Consider the following expression
\[
    f(x_0 + h) - f(x_0) = h f^{\prime}(x_0) + \varepsilon(h) 
\]
By comparing this with the definition of the derivative, we can take a limit
\begin{align*}
    \lim_{h \rightarrow 0}{\frac{f(x_0 + h) - f(x_0)}{h}} &= f^{\prime}(x_0) + \lim_{h \rightarrow 0}{\varepsilon(h)} \\
    &\therefore  \lim_{h \rightarrow 0}{\varepsilon(h)} = 0 \\
    &\Rightarrow \varepsilon(h) = \underline{o}(h)
\end{align*}
\[
    \therefore f(x_0 + h) = f(x_0) + h f^{\prime}(x_0) + \underline{o}(h)
\]

%%%%%%%%%%%%%%%%%%%%%%
% Lecture 2: 8/10/2023
%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Rules for differentiation}
\begin{thm}[Chain Rule]
    Given $f(x) = F(g(x))$,
    \[
        \diff{f}{x} = F^{\prime}(g(x)) \cdot \diff{g}{x}   = \diff{F}{g}\diff{g}{x}
    \]
    Note that $F^{\prime}(g(x))$ refers to the derivative of the function $F$ with respect to its argument $g$ and then evaluated at $g(x)$
\end{thm}

\begin{proof}[See notes]
    
\end{proof}

\begin{eg}
    \[
        \diff{}{x} \sin{(x^2 -x +2)} = \cos{(x^2 -x +2)}[2x - 1]
    \]
\end{eg}

\begin{thm}[Product Rule]
    Given $f(x) = u(x)v(x)$,
    \[
        \diff{f}{x} = v \diff{u}{x} + u \frac{dv}{dx}
    \]
    The Quotient rule is a special case of the product rule, when $v \rightarrow \frac{1}{v}$
    \begin{align*}
        \diff{}{x}\left(\frac{u}{v}\right) &= \frac{1}{v}\diff{u}{x} + u \diff{}{x}\left(\frac{1}{v}\right) \\
        &= \frac{1}{v}\diff{u}{x} + \frac{u}{v^2} \frac{dv}{dx} \\
        &= \frac{vu^{\prime} - v^{\prime}u}{v^2}
    \end{align*}
\end{thm}

\begin{thm}[Leibnitz'z Rule]
    Consider $f(x) = u(x)v(x)$ then,
    \begin{align*}
        f^{\prime} &= u^{\prime}v + uv^{\prime} \\
        f^{\prime\prime} &= u^{\prime\prime}v + 2u^{\prime}v^{\prime} + uv^{\prime\prime} \\
        f^{\prime\prime\prime} &= u^{\prime\prime\prime}v + 3u^{\prime\prime}v^{\prime} + 3u^{\prime}v^{\prime\prime} + uv^{\prime\prime\prime} \\
        \vdots
    \end{align*}

    We have that
    \[
        f^{(n)}(x) = \sum_{r=0}^{n}{\binom{n}{k}u^{(n-r)}(x)v^{r}(x)}
    \]
    Note that
    \[
        u^{(0)}(x) \equiv u(x)  
    \]
\end{thm}

\begin{proof}[Exercise by induction]
    
\end{proof}

\subsubsection{Taylor Series}
A taylor series is an infinite series that tries to approximate a function in the vincity of some point. It is constructed to match the first $n$ differentials of a function.

\begin{defi}[Taylor Series]
    For $f(x)$ infinitely differentiable at $x_0$, the taylor series $T_f(x)$ is defined to be
    \[
        T_f(x) = f(x_0) + (x - x_0)f^{\prime}(x) + \frac{(x - x_0)^2}{2!}f^{\prime\prime}(x) + \cdots + \frac{(x - x_0)^n}{n!}f^{(n)}(x) + \cdots
    \]
\end{defi}

Taylor series are often used for approximations. To consider the errors these approximations have, we look at Taylor's Theorem.

\begin{thm}[Taylor's Theorem]
    For $n$-times differentiable $f(x)$ at $x_0$,
    \[
        f(x_0 + h) = f(x_0) + hf^{\prime}(x_0) + \frac{h^2}{2!}f^{\prime\prime}(x_0) + \cdots + \frac{h^n}{n!}f^{(n)}(x) + E_n
    \]
    Where $E_n$ accounts for the error in the approximation. Taylor's theorem states that
    \[
        E_n = \underline{o}(h^n) \quad \text{as} \quad x \rightarrow 0  
    \]
\end{thm}

A stronger form of this exists, if $f^{(n+1)}(x)$ exists $\forall x \in [x_0, x_0 + h]$ as in continuous in this range,
\begin{align*}
    E_n &= O(h^{n+1}) \quad \text{as} \quad x \rightarrow 0 \\
    &= \frac{f^{n+1}(x_n)}{(n+1)!}h^{n+1} \quad \text{for} \quad x \in [x_0, x_0 +h]
\end{align*}
This is due to Lagrange. Note that $E_n = O(h^{n+1})$ is stronger than $\underline{o}(h^n)$. For instance, consider $h^{n + 0.5} = \underline{o}(h^n)$ but $\neq O(h^{n+1})$ as $h \rightarrow 0$

\begin{defi}[Taylor Polynomials]
    With $x = x_0 + h$ and Taylor's theorem gives
    \[
        f(x) = f(x_0) + (x - x_0)f^{\prime}(x) + \frac{(x - x_0)^2}{2!}f^{\prime\prime}(x) + \cdots + \frac{(x - x_0)^n}{n!}f^{(n)}(x) + E_n
    \]
    This is called the ``$n^{th}$'' Taylor polynomial about $x_0$. It is constructed to match the first $n$ derivatives of $f(x)$ as $x_0$ and provides a local approximation to $f(x)$ in the vicinity of $x_0$ with error $E_n = O(h^{n+1})$. If we have that
    \[
        \lim_{n \rightarrow \infty}{E_n} = 0  
    \]
    then the Taylor Polynomial converges to $f(x)$
\end{defi}

\subsubsection{L'H\^opital's Rule}
This theorem allows us to deal with limits of \emph{indeterminant forms}. For example $\lim_{x \rightarrow x_0}{\frac{f(x)}{g(x)}}$ where $\lim_{x \rightarrow x_0}{f(x)} = \lim_{x \rightarrow x_0}{g(x)} = 0$.

\begin{thm}[L'H\^opital's Rule]
    Let $f(x)$ and $g(x)$ be differentiable at $x_0$ and have continuous first derivatives there, and
    \begin{align*}
        \lim_{x \rightarrow x_0}{f(x)} &= f(x_0) = 0 \\
        \lim_{x \rightarrow x_0}{g(x)} &= g(x_0) = 0
    \end{align*}
    Then if $g^{\prime}(x_0) \neq 0$,
    \[
        \lim_{x \rightarrow x_0}{\frac{f(x)}{g(x)}} = \lim_{x \rightarrow x_0}{\frac{f^{\prime}(x)}{g^{\prime}(x)}}
    \]
    Provided that the second limit exists. Note that the non-existence of the second limit does not give any information about the original limit.
\end{thm}

\begin{proof}
    Using the result at the end of the section discussing order parameters,
    \begin{align*}
        f(x) &= f(x_0) + (x - x_0)f^{\prime}(x_0) + \underline{o}(x - x_0) \quad \text{as} \quad x \rightarrow x_0 \\
        g(x) &= g(x_0) + (x - x_0)g^{\prime}(x_0) + \underline{o}(x - x_0) \quad \text{as} \quad x \rightarrow x_0
    \end{align*}
    \[
        \lim_{x \rightarrow x_0}{\frac{f(x)}{g(x)}} = \lim_{x \rightarrow x_0}{\frac{f^{\prime}(x_0) + \frac{\underline{o}(x - x_0)}{x - x_0}}{g^{\prime}(x_0) + \frac{\underline{o}(x - x_0)}{x - x_0}}}
    \]
    Since we have that $g^{\prime}(x_0) \neq 0$, 
    \begin{align*}
        \lim_{x \rightarrow x_0}{\frac{f(x)}{g(x)}} &= \frac{f^{\prime}(x_0)}{g^{\prime}(x_0)} \\
        &= \frac{\lim_{x \rightarrow x_0}{f^{\prime}(x)}}{\lim_{x \rightarrow x_0}{g^{\prime}(x)}} \tag{assuming continuous first derivatives}\\
        &= \lim_{x \rightarrow x_0}{\frac{f^{\prime}(x)}{g^{\prime}(x)}} \tag{already checked $g(x_0) \neq 0$}
    \end{align*}
\end{proof}

We can generalise, by repeating L'\^opital's Rule, given that the conditions hold. For instance, we can have if $f^{\prime}(x_0) = g^{\prime}(x_0) = 0$
\[
    \lim_{x \rightarrow x_0}{\frac{f(x)}{g(x)}} = \frac{f^{\prime\prime}(x_0)}{g^{\prime\prime}(x_0)}
\]

\begin{eg}
    \begin{align*}
        f(x) &= 3\sin{x} - \sin{3x}
        g(x) = 2x - \sin{2x}
    \end{align*}
\end{eg}

%%%%%%%%%%%%%%%%%%%%%%
% Lecture 3: 11/10/2023
%%%%%%%%%%%%%%%%%%%%%%

\subsection{Integration}
\subsubsection{Reimann sums}
We will formalise the idea of taking the area under a function. 

\begin{defi}[Integral]
    The integral of a suitably well-defined function $f(x)$ is the limit of a sum
    \begin{align*}
        \int_{a}^{b}{f(x) \ \diffd x} &\equiv \lim_{N \rightarrow \infty}{\sum_{n=0}^{N-1}{f(x_n)\Delta x}} \\
        &= \lim_{\Delta x \rightarrow 0}{\sum_{n=0}^{N-1}{f(x_n)\Delta x}}
    \end{align*}
    Where $\Delta x = \frac{b - a}{N}$ and $x_n = a + n\Delta x$
    
    % Insert diagram of reimann sums
    Note that the $\lim_{N \rightarrow \infty}$ is independant on how the rectangles are drawn. For instance, we can have $\Delta x$ be non-uniform. (see Analysis I)
\end{defi}

As we have drawn, it appears that each partial sum will underestimate the limit of the area.
\begin{prop}
    We will show that in the limit as the $\Delta x \rightarrow 0$, that the difference between the area and the limit approaches $0$.
\end{prop}
\begin{proof}
    Consider one rectangle for finite $N$. 
    % insert diagram
    Using the mean value theorem, there exists a $c$ in the range $x_n \leq c \leq x_{n+1}$ such that the area $A_n$
    \[
        A_n = (x_{n+1} - x_n)f(c)  
    \]
    If $f(x)$ is differentiable we have from Taylor's theorem that
    \[
        f(c) = f(x_n) + O(c - x_n), \quad \text{as} \ c-x_n \rightarrow 0  
    \]
    Since $\Delta x$ is larger than $c - x_n$,
    \[
        f(c) = f(x_n) + O(\Delta x)
    \]
    Thus it follows that
    \[
        A_n =  \Delta x f(x_n) + \Delta x O(\Delta x), \quad \text{as} \ \Delta x \rightarrow 0 
    \]
    Note that
    \[
        \Delta x O(\Delta x) = O(\Delta x^2)
    \]
    The total area between $x=a$ and $x=b$ is therefore,
    \[
        A = \lim_{N \rightarrow \infty}{\sum_{n=0}^{N-1}{A_n}} = \lim_{N \rightarrow \infty}{\sum_{n=0}^{N-1}{\Delta x f(\Delta x)}} + \lim_{N \rightarrow \infty}{\sum_{n=0}^{N-1}{O(\Delta x^2)}}
    \]
    Note that
    \[
        \lim_{N \rightarrow \infty}{\sum_{n=0}^{N-1}{\Delta x f(\Delta x)}}
    \]
    Is the definition of the reimann sum and therefore the error is given by
    \begin{align*}
        \lim_{N \rightarrow \infty}{\sum_{n=0}^{N-1}{O(\Delta x^2)}} &= \lim_{N \rightarrow \infty}{NO \bkt{\bkt{\frac{b-a}{N}}^2}} \\
        &= \lim_{N \rightarrow \infty}{O\bkt{\frac{(b-a)^2}{N}}} \\
        &= 0
    \end{align*}
\end{proof}

\subsubsection{Fundamental Theorem of Calculus}
We will formalise the idea of taking the inverse of differentiation.

\begin{thm}[FTC]
    Let $F(x)$ be defined as
    \[
        F(x) = \int_{a}^{x}{f(t) \ \diffd t}  
    \]
    Then,
    \[
        \diff{F}{x} = f(x)
    \]
\end{thm}
\begin{proof}
    \begin{align*}
        \diff{F}{x} &= \lim_{h \rightarrow 0}\frac{F(x + h) - F(x)}{h} \\
        &= \lim_{h \rightarrow 0}\frac{1}{n}\sqbkt{\int_{x}^{x+h}{f(t) \ \diffd t}} \\
        &= \lim_{h \rightarrow 0}\frac{1}{n}\sqbkt{f(x)h + O(h^2)} \tag{from reimann sum definition} \\
        &= f(x) + \lim_{h \rightarrow 0}O(h^2) \\
        &= f(x)
    \end{align*}
\end{proof}

\begin{remark}
    $F(x)$ is a solution of the differential equation $\diff{F}{x} = f(x)$ with the added property that $F(a) = 0$
\end{remark}

\begin{cor}
\[
    \diff{}{x}\int_{x}^{b}{f(x) \ \diffd t} = -f(x)
\]
and by the chain rule, 
\[
    \diff{}{x}\int_{a}^{g(x)}{f(x) \ \diffd t} = f(g(x))g'(x)
\]
\end{cor}

\begin{notation}
    We typically write indefinite integrals as $\int{f(x) \ \diffd x}$. We may also write
    \[
        \int^{x}{f(t) \ \diffd t}    
    \]
    wich is arguably better as it makes clear that the result is a function of $x$. Further note that the undefined lower limit leaves space for the constant of integration $c$.
\end{notation}

\subsubsection{Methods of Integration}

\begin{defi}[Substitution]
    Substitution is useful if the integrand contains composed functions, or if we can spot the derivative of a function and itself.
\end{defi}
\begin{eg}
    \[
        I = \int{\frac{1 - 2x}{\sqrt{x - x^2}} \ \diffd x}
    \]
    Let $u = x - x^2$, $\diffd u = (1 - 2x) \diffd x$
    \begin{align*}
        I &= \int{\frac{1}{\sqrt{u}} \ \diffd u} \\
        &= 2\sqrt{u} \\
        &= 2\sqrt{x - x^2} + x
    \end{align*}
\end{eg}
Here are some useful substitution to consider, motivated by the trigonometric and hyperbolic identities
\begin{center}
    \begin{tabular}{c | c}
        Term in integrand & Substitution \\
        \midrule
        $1 - x^2$ & $x = \sin{\theta} or \tanh{\theta}$\\
        $1 + x^2$ & $x = \tan{\theta} or \sinh{\theta}$\\
        $x^2 - 1$ & $x = \sec{\theta} or \cosh{\theta}$\\

    \end{tabular}
\end{center}

\begin{eg}
    \[
        I = \int{\sqrt{x - x^2} \ \diffd x} = \int{\sqrt{1 - (x - 1)^2} \ \diffd x}    
    \]
    Let $x - 1 = \sin{\theta}$, $\diffd x = \cos{\theta} \diffd \theta$
    \begin{align*}
        I &= \int{\cos^2{\theta} \ \diffd \theta} \\
        &= \int{\frac{\cos{2\theta} + 1}{2}} \ diffd \theta \\
        &= \frac{1}{4}\sin{2\theta} + \frac{1}{2}\theta + c \\
        &= \frac{1}{2}\sin^{-1}{(x-1)} + \frac{1}{2}(x-1)\sqrt{2x - x^2} + c
    \end{align*}
\end{eg}

\begin{defi}[By parts]
    Integration by parts follows from the product rule. Recall
    \[
        (uv)' = uv' + u'v
    \]
    We can then rearrange
    \[
        uv' = (uv)' - u'v \Rightarrow \int{uv' \diffd x} = uv - \int{u'v \diffd x}
    \]
\end{defi}

\begin{eg}\leavevmode
    \begin{enumerate}
        \item \begin{align*}
            I = \int_{0}^{\infty}{xe^{-x} \diffd x} &= \left. -xe^{-x} \right\rvert_{0}^{\infty} + \int_{0}^{\infty}{e^-x} \\
            &= 0 + 1 = 1
        \end{align*}
        \item \[
            I = \int{\ln{x} \diffd x} = x\ln{x} - x + c    
        \]
    \end{enumerate}
\end{eg}

%%%%%%%%%%%%%%%%%%%%%%
% Lecture 4: 13/10/2023
%%%%%%%%%%%%%%%%%%%%%%

\begin{remark}
    We will briefly give an outline of the proof of taylor's theorem
\end{remark}
\begin{proof}
    Using FTC, we can expand
    \[
        f(x) = f(0) + \int_{0}^{x}{f'(t) \diffd t}
    \]
    Then, we insert a factor of $1 = -\frac{d(x-t)}{dt}$ and integrate by parts,
    \begin{align*}
        f(x) &= f(0) + \int_{0}^{x}{f'(t) \diffd t} \\
        &= f(0) + \int_{0}^{x}{-\frac{d(x-t)}{dt} \times f'(t) \diffd t} \\
        &= f(0) + \int_{0}^{x}{-\frac{d(x-t)}{dt} \times f'(t) \diffd t} \\
        &= f(0) - (x-t)f'(t) \Big{|}_{0}^{x} - \int_{0}^{x}{-(x-t) \times f''(t) \diffd t} \\
        &= f(0) + xf'(0) + \int_{0}^{x}{(x-t) \times f''(t) \diffd t}
    \end{align*}
    We can continue this expansion, replacing $(x - t) = -\frac{1}{2}\diff{}{t}(x-t)^2$
    \begin{align*}
        f(x) &= f(0) + xf'(0) + \int_{0}^{x}{(x-t) \times f''(t) \diffd t} \\
        &= f(0) + xf'(0) + \int_{0}^{x}{-\frac{1}{2}\diff{(x-t)^2}{t} \times f''(t) \diffd t} \\
        &= f(0) + xf'(0) - \frac{1}{2}(x-t)^2 f''(t) \Big{|}_{0}^{x} - \int_{0}^{x}{-\frac{1}{2}(x-t)^2 \times f'''(t) \diffd t} \\
        &= f(0) + xf'(0) + \frac{1}{2}x^2 f''(0) + \frac{1}{2}\int_{0}^{x}{(x-t)^2 \times f'''(t) \diffd t}
    \end{align*}
    In general, we will be replacing terms of the form $(x - t)^n$ with $-\frac{1}{n+1} \diff{}{t}(x - t)^{n+1}$. Doing so with return the first $n$ terms of the taylor expansion,
    \[
    f(x) = f(0) + xf'(0) + \frac{x^2}{2} f''(0) + \ldots + \frac{x^n}{n!}f^{(n)}(0) + \frac{1}{n!} \int_{0}^{2}{(x-t)^n f^{(n+1)}(t) \diffd t}
    \]
    Therefore, we recover an integral form of the error term
    \[
        E_n = \frac{1}{n!} \int_{0}^{2}{(x-t)^n f^{(n+1)}(t) \diffd t}
    \]
    Using the mean value theorem, we can find a value $x_n$ in the interval that satisfies the intergral
    \[
        E_n = \frac{f^{(n+1)}(x_n)}{n!} \int_{0}^{2}{(x-t)^n \diffd t} = \frac{f^{(n+1)}(x_n)}{(n+1)!} x^{n+1}
    \]
    where $0 \leq x_n \leq x$
\end{proof}

\subsection{Partial Differentiation}
\subsubsection{Partial derivatives}
In this chapter, we will generalise differentiation to function of more than one variable. For instance, we may consider the height of terrain as a function of longitude and latitude, or the density of air as a function of position and time. 
Consider the first example, in the diagram we plot the contours of the function on the $x-y$ plane (the curves for which the height is constant).
% insert diagram

Notice that the slop of the function at some point $A$ depends on the direction. Therefore, we will find the slope of the graph along the $x$ and $y$ direction independantly, where we will be able to combine the them to find the slopes in any direction.
\begin{defi}[Partial derivative]
    Given a function of several variables, $f(x, y)$ we define the \emph{partial derivative} of $f$ w.r.t. $x$ at fixed $y$ given by
    \[
        \diffp{f}{x} \Big{|}_{y} = \lim_{\delta x \rightarrow 0}{\frac{f(x + \delta x, y) - f(x, y)}{\delta x}} 
    \]
    This limit therefore find the slope of $f$ when moving in the positive $x$ direction. Similarly, we may define
    \[
        \diffp{f}{y} \Big{|}_{x} = \lim_{\delta y \rightarrow 0}{\frac{f(x, y + \delta y) - f(x, y)}{\delta y}} 
    \]
\end{defi}

\begin{remark}
    For suitably smooth functions we can also define higher derivates,
    \[
        \diffp{f}{{x^2}} = \diffp{}{x}\bkt{\diffp{f}{x}} \quad \ldots \quad \diffp[n]{f}{x} = \diffp{}{x}\bkt{\diffp[n-1]{f}{x}}
    \]
    as well as mixed partial derivatives,
    \[
        \diffp{}{y}\bkt{\diffp{f}{x}} = \diffp{f}{{y}{x}}
    \]
    where the order of the partial derivatives taken starts from the right.
\end{remark}

\begin{eg}[$f(x) = x^2 + y^2 + e^{xy^2}$]
    We can compute first derivatives
    \[
        \diffp{f}{x} \Big{|}_{y} = 2x + y^2e^{xy^2} \qquad \diffp{f}{y} \Big{|}_{x} = 2y + 2xye^{xy^2}
    \]
    and similarly for second derivatives
    \[
        \diffp[2]{f}{x} \Big{|}_{y} = 2 + y^4e^{xy^2} \qquad \diffp[2]{f}{y} \Big{|}_{x} = 2 + 2xe^{xy^2} + (2xy)^2e^{xy^2}
    \]
    as well as the mixed partial derivatives
    \begin{align*}
        \diffp{}{x}\bkt{\diffp{f}{y} \Big{|}_{x}} \Big{|}_{y} &= 2ye^{xy^2} + 2xy^3e^{xy^2} \\
        \diffp{}{y}\bkt{\diffp{f}{x} \Big{|}_{y}} \Big{|}_{x} &= 2ye^{xy^2} + 2xy^3e^{xy^2} \\
    \end{align*}
    Note that the mixed partial derivates are equaiavalent. In cases where the function has second partial derivatives, mixed second partial derviatives will be the same. This is known as Schwarz's Theorem.
\end{eg}

\begin{notation}
    As a useful convension, we will omit the $\Big{|}_{x}$, and work on the assumption that all other variables are held constant. There are also some alternative forms,
    \[
        f_x = \diffp{f}{x} \qquad f_{xy} = \diffp{f}{{y}{x}}  
    \]
\end{notation}
\subsubsection{Multivariate chain rule}
As we have the chain rule for regular differentiation, we are interested in extending this to partial derivatives. Consider the following question. Suppose I am walking around some terrain. What is the derivative of my height with respect to time.

Given a path $x(t), y(t)$ and some function $f(x(t), y(t))$, what is $\diff{f}{t}$? We consider a change in $f$ when we move from 
\[
    (x, y) \mapsto (x + \delta x, y + \delta y)    
\]
We can expand out, and include a term that "bridges" the gap
\begin{align*}
    \delta f = f(x + \delta x, y + \delta y) & &- f(x, y) \\
    = f(x + \delta x, y + \delta y) &- f(x + \delta x, y) \\
    &+f(x + \delta x, y) &- f(x, y)
\end{align*}
Applying Taylors expansion we have,
\[
    f(x + \delta x, y) - f(x, y) = f_x(x, y)\delta x + \ltlo{\delta x}
\]
as well as,
\[
    f(x + \delta x, y + \delta y) - f(x + \delta x, y) = f_y(x + \delta x, y)\delta y + \ltlo{\delta y}
\]
where we can further expand
\[
    f_y(x + \delta x, y) = f_y(x, y) + f_{yx}(x, y)\delta x + \ltlo{\delta x}
\]
Together,
\[
    \delta f = f_x(x, y)\delta x + \ltlo{\delta x} + \sqbkt{f_y(x, y) + f_{yx}(x, y)\delta x + \ltlo{\delta x}}\delta y + \ltlo{\delta y}
\]
Note that taylors theorem allowed us to remove the dependance of $\delta x$ and $\delta y$ from the function. We can now use the differential of $f$.

\begin{defi}[Differential]
    The \emph{differential} of $f$ is defined to be
    \[
        \diffd f = \lim_{\substack{\delta x \rightarrow 0 \\ \delta y \rightarrow 0}} \delta f  
    \]
\end{defi}
Applying this get
\begin{align*}
    \diffd f &= \lim_{\substack{\delta x \rightarrow 0 \\ \delta y \rightarrow 0}} f_x(x, y)\delta x + \ltlo{\delta x} + f_y(x, y)\delta y + f_{yx}(x, y)\delta x\delta y + \ltlo{\delta x}\delta y + \ltlo{\delta y} \\
    &= f_x(x, y) \lim_{\delta x \rightarrow 0} \delta x + f_y(x, y) \lim_{\delta y \rightarrow 0}\delta y \\
    &= f_x(x, y) \diffd x + f_y(x, y) \diffd y
\end{align*}

\begin{thm}[Chain rule for partial differentiation]
    The differential of $f(x, y)$ is related to the differential of its arguments by,
    \[
        \diffd f = \diffp{f}{x} \diffd x + \diffp{f}{y} \diffd y
    \] 
    and more generally, for a function $f$ whose arguments are $x_1, x_2, \cdots, x_n$,
    % \[
    %     \diffd f = \diffp{f}{x_1} \diffd x_1 + \ldots + \diffp{f}{x_n} \diffd x_n
    % \]
    \[
        \diffd f = \diffp{f}{{x_1}} \diffd x_1 + \ldots + \diffp{f}{{x_n}} \diffd x_n
    \]
\end{thm}

Hence for the path $x(t), y(t)$
\[
    \diff{f}{t} = \diffp{f}{x} \diff{x}{t} + \diffp{f}{y} \diff{y}{t}
\]
Furthermore, if the path is parameterised by a coordinate, for instance $y = f(x)$, then we simply have a function in one variable and
\[
    \diff{f}{x} = \diffp{f}{x} + \diffp{f}{y} \diff{y}{x}
\]

\begin{defi}[Integral form]
    If we define $\Delta f$ as the change in height between the two endpoints we have,
    \[
        \Delta f = \int{\diffd f} = \int{\bkt{\diffp{f}{x} \diffd x + \diffp{f}{y} \diffd y}}  
    \]
    In the case that $x(t)$ and $y(t)$,
    \[
        \Delta f = \int{\diffd f} = \int{\bkt{\diffp{f}{x} \diff{x}{t} + \diffp{f}{y} \diff{y}{t}} \diffd t}  
    \]
\end{defi}
\subsubsection{Applications of multivariate chain rule}

We can use the multivariate chain rule to help us change variables. For instance consider the map from cartesian coordintes to plane-polar coordinates.
\[
    (x, y) \mapsto (r, \theta)
\]

% insert diagram
Then we have the relationships
\[
    x = r\cos{\theta} \qquad y = r\sin{\theta}
\]
and we can reinterprit the function $f(x, y)$ as a function $f(x(r, \theta), y(r, \theta))$ in the plane-polar coordinates. In this case,
\begin{align*}
    \diffp{f}{r} \Big{|}_{\theta} &= \diffp{f}{x}\diffp{x}{r}\Big{|}_{\theta} + \diffp{f}{y}\diffp{y}{r}\Big{|}_{\theta} \\
    &= \diffp{f}{x}\cos{\theta} + \diffp{f}{y}\sin{\theta}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%
% Lecture 5: 16/10/2023
%%%%%%%%%%%%%%%%%%%%%%

\begin{defi}[Implicit differentiation]
    Consider $f(x, y, z) = c$ where $c$ is a constant. $f$ defines a surface in 3D space, implicityly defining
    \[
        x = x(y, z) \quad y = y(x, z) \quad z = z(x, y)  
    \]
    By implicit we mean that it may not be possible to write out these relationships explicitly. We can still evaluate their partial derivatives. Generally,
    \[
        \diffd f = \diffp{f}x{x}\Big{|}_{y,z} \diffd x + \diffp{f}x{y}\Big{|}_{x,z} \diffd y + \diffp{f}x{z}\Big{|}_{x,y} \diffd z
    \]
\end{defi}

\begin{eg}
    \[
        f: xy + y^2z + z^5 = 1  
    \]
    Notice that we may be able to write out forms for $x(y, z)$ and $y(x, z)$ but it will not be possible to do so for $z$ as we have a qunitic! Take the derivative with respect to $x$ while holding $y$ fixed.
    \begin{align*}
        y + y^2 \diffp{z}{x}\Big{|}_{y} + 5z \diffp{z}{x}\Big{|}_{y} = 0
        \Rightarrow \diffp{z}{x}\Big{|}_{y} = \frac{-y}{y^2 + 5z^4}
    \end{align*}
\end{eg}

\begin{remark}
    Note that for any surface when $f$ is constant, we have the contraint that
    \[
        \diffd f = 0  
    \]
    and therefore we cannot independantly vary $x$, $y$ and $z$ without leaving the surface.
\end{remark}

\begin{cor}[Euler's chain rule/cyclic rule]
    We may find the rate of change of $z$ with respect to $x$ at fixed $y$
    \begin{align*}
        0 &= \diffp{f}x{x}\Big{|}_{y,z} \diffp{x}{x}\Big{|}_{y} + \diffp{f}x{y}\Big{|}_{x,z} \diffp{y}{x}\Big{|}_{y} + \diffp{f}x{z}\Big{|}_{x,y} \diffp{z}{x}\Big{|}_{y} \\
        &= \diffp{f}x{x}\Big{|}_{y,z} + \diffp{f}x{z}\Big{|}_{x,y} \diffp{z}{x}\Big{|}_{y} \\
        \Rightarrow& \diffp{z}{x}\Big{|}_{y} = \frac{-\diffp{f}{x}\Big{|}_{y,z}}{\diffp{f}{z}\Big{|}_{x,y}}
    \end{align*}
    and similarly,
    \[
        \diffp{x}{y}\Big{|}_{z} = \frac{-\diffp{f}{y}\Big{|}_{x,z}}{\diffp{f}{x}\Big{|}_{y, z}} \qquad \diffp{y}{z}\Big{|}_{x} = \frac{-\diffp{f}{z}\Big{|}_{x,y}}{\diffp{f}{y}\Big{|}_{x,z}}
    \]
    if we multiply these three, we get pairwise cancellation and arrive at for 3 indepdnant variables,
    \[
        \diffp{x}{y}\Big{|}_{z} \times \diffp{y}{z}\Big{|}_{x} \times \diffp{z}{x}\Big{|}_{y} = -1
    \]
    Note to self: judging by the construction it will be $(-1)^n$ where $n$ is the number of variables.
\end{cor}

\begin{defi}[Reciprocal Rule]
    We may similarly find
    \[
        \diffp{x}{z}\Big{|}_{y} = \frac{-\diffp{f}{z}\Big{|}_{x,y}}{\diffp{f}{x}\Big{|}_{y,z}}
    \]
    Which gives us
    \[
        \diffp{z}{x}\Big{|}_{y} = \frac{1}{\diffp{x}{z}\Big{|}_{y}}
    \]
\end{defi}

\begin{thm}[Differentiating an integral w.r.t. a parameter]
    Given an integral parametersed by some $\alpha$ whose end points may be dependant upon $\alpha$
    \[
        I(\alpha) = \int_{a(\alpha)}^{b(\alpha)}{f(x ; \alpha) \diffd x}
    \]
    We may differentiate with respect to $\alpha$ to get
    \[
        \diff{I}{\alpha} = \int_{a(\alpha)}^{b(\alpha)}{\diffp{f(x ; \alpha)}{\alpha} \diffd x} + f(b(\alpha); \alpha) \diff{b}{\alpha} - f(a(\alpha); \alpha) \diff{a}{\alpha}
    \]
    which may be thought of as partial differentiation under the intergral followed by evaluating the bounds while using the chain rule. 
\end{thm}

\begin{proof}
    \begin{align*}
        \diff{I}{\alpha} =& \lim_{\delta \alpha \rightarrow 0} \frac{1}{\delta \alpha} \left[\int_{a(\alpha + \delta \alpha)}^{b(\alpha + \delta \alpha)}{f(x ; \alpha + \delta \alpha) \diffd x} - \int_{a(\alpha)}^{b(\alpha)}{f(x ; \alpha) \diffd x}\right] \\
        =&\lim_{\delta \alpha \rightarrow 0} \frac{1}{\delta \alpha} \left[\int_{a(\alpha)}^{b(\alpha)}{f(x ; \alpha + \delta \alpha) \diffd x} +\right. \int_{b(\alpha)}^{b(\alpha + \delta \alpha)}{f(x ; \alpha + \delta \alpha) \diffd x} \\ 
        &\left. - \int_{a(\alpha)}^{a(\alpha + \delta \alpha)}{f(x ; \alpha + \delta \alpha) \diffd x} \right] - \int_{a(\alpha)}^{b(\alpha)}{f(x ; \alpha) \diffd x} \\
        =&\lim_{\delta \alpha \rightarrow 0} \frac{1}{\delta \alpha} \left[ \int_{a(\alpha)}^{b(\alpha)}{f(x ; \alpha + \delta \alpha) - f(x ; \alpha) \diffd x} + \int_{b(\alpha)}^{b(\alpha + \delta \alpha)}{f(x ; \alpha + \delta \alpha) \diffd x}\right.\\
        &-  \left.\int_{a(\alpha)}^{a(\alpha + \delta \alpha)}{f(x ; \alpha + \delta \alpha) \diffd x} \right]
    \end{align*}
    The first integral is equivalent to partial differentiation of $f$
    \[
        \lim_{\delta \alpha \rightarrow 0} \frac{1}{\delta \alpha} \int_{a(\alpha)}^{b(\alpha)}{f(x ; \alpha + \delta \alpha) - f(x ; \alpha) \diffd x} = \int_{a(\alpha)}^{b(\alpha)}{\diffp{f(x ; \alpha)}{\alpha} \diffd x}
    \]
    For the second integral we use the mean value theorem. Using MVT, we find that $\exists \overline{x} \quad b(\alpha) \leq \overline{x} \leq b(\alpha + \delta \alpha)$ such that,
    \[
        \int_{b(\alpha)}^{b(\alpha + \delta \alpha)}{f(x ; \alpha + \delta \alpha) \diffd x} = [b(\alpha + \delta \alpha) - b(\alpha)]f(\overline{x} ; \alpha + \delta \alpha)
    \]
    taking the limit we get
    \begin{align*}
        \lim_{\delta \alpha \rightarrow 0} \int_{b(\alpha)}^{b(\alpha + \delta \alpha)}{f(x ; \alpha + \delta \alpha) \diffd x} =& \lim_{\delta \alpha \rightarrow 0} \frac{b(\alpha + \delta \alpha) - b(\alpha)}{\delta \alpha}f(\overline{x} ; \alpha + \delta \alpha) \\
        &= \diff{b}{\alpha}f(b(\alpha); \alpha)
    \end{align*}
    since in the limit, $\overline{x} \rightarrow b(\alpha)$, and similarly for the third integral.
\end{proof}

\begin{eg}\leavevmode
    \begin{enumerate}
        \item $I(\lambda) = \int_{0}^{\lambda}{e^{-\lambda x^2} \diffd x}$
        \[
            \diff{I}{\lambda} = \int_{0}^{\lambda}{-x^2e^{-\lambda x^2} \diffd x} + \diff{\lambda}{\lambda}e^{-\lambda^3} - 0
        \]
        \item $\int_{0}^{\infty}{x^ne^{-x} \diffd x}$. We can sneak in a paramter
        \begin{align*}
            \text{Let} &\ I(\lambda) = \int_{0}^{\infty}{e^-\lambda x \diffd x} = \frac{1}{\lambda} \\
            &\Rightarrow \diff[n]{I}{\lambda} = \int_{0}^{\infty}{(-1)^n x^n e^{-\lambda x} \diffd x} = \frac{(-1)^n n!}{\lambda^{n+1}} \tag{By differentiating $\frac{1}{\lambda}$} \\
            &\Rightarrow \int_{0}^{\infty}{x^ne^-x \diffd x} = n! \tag{Set $\lambda = 1$}
        \end{align*}
    \end{enumerate}
\end{eg}

\section{First-Order Linear Differential Equations}
In general there are two flavours of Differential Equations,
\begin{enumerate}
    \item Ordinary: Involving functions of one variable
    \item Partial: Involving functions of many variables
\end{enumerate}
in this course, we will focus on ordinary differential equations and maybe do abit of partial differential equations at the end.
\subsection{Exponential Function}

\begin{defi}[Exponential Function]
    Exponential functions play a key role in solutions to linear ODEs. We define it as the infinite series.
    \begin{align*}
        \exp(x) &\equiv 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots + \frac{x^n}{n!} + \cdots \\
        &= \sum_{i=0}^{\infty}{\frac{x^n}{n!}}
    \end{align*}
    It can also be written as a limit of the form
    \[
        \exp(x) = \lim_{k \rightarrow \infty} \bkt{1 + \frac{x}{k}}^k  
    \]
\end{defi}

\begin{proof}
    To show that the two forms are equivalent we can use the binomail theorem.\
    \begin{align*}
        \lim_{k \rightarrow \infty} \bkt{1 + \frac{x}{k}}^k = \lim_{k \rightarrow \infty} \bkt{1 + k \frac{x}{k} + \frac{k(k-1)}{2!}\bkt{\frac{x}{k}}^2 + \cdots} 
    \end{align*}
    Where in the limit,
    \begin{align*}
        \lim_{k \rightarrow \infty} \binom{k}{n} \bkt{\frac{x}{k}}^n &= \frac{k! x^n}{n!(k - n)! k^n} \\
        &= \frac{x^n}{n!} \lim_{k \rightarrow \infty} \frac{k!}{(k - n)! k^n} \\
        &= \frac{x^n}{n!} \lim_{k \rightarrow \infty} \frac{k}{k} \frac{k-1}{k} \frac{k-2}{k} \cdots \frac{k-(n -1)}{k} \\
        &= \frac{x^n}{n!}
    \end{align*}
\end{proof}

We may also differentiate the series to find,
\begin{align*}
    \diff{}{x}\exp(x) &= 1 + \frac{2x}{2!} + \frac{3x^2}{3!} + \cdots \\
    &= 1 + x + \frac{x^2}{2!} + \cdots \\
    &= \exp(x)
\end{align*}
Which is an important property that we will use to solve ODEs with. As $\exp(0) = 1$, we can think if $\exp(x)$ as a solution of the ODE,
\[
    \diff{f}{x} = f \quad \text{and} \quad f(0) = 1  
\]

Another key property of $\exp(x)$ is:
\[
    \exp(x_1)\exp(x_2) = \exp(x_1 + x_2)
\]
We have seen a proof of this via the vectors and matrices course, however, let's do it another way using the limit definition.
\begin{align*}
    \exp(x_1)\exp(x_2) =& \lim_{k \rightarrow \infty} \bkt{1 + \frac{x_1}{k}}^k \lim_{k \rightarrow \infty} \bkt{1 + \frac{x_2}{k}}^k \\
    =& \lim_{k \rightarrow \infty} \sqbkt{\bkt{1 + \frac{x_1}{k}} \bkt{1 + \frac{x_2}{k}}}^k \\
    =& \lim_{k \rightarrow \infty} \sqbkt{1 + \bkt{\frac{x_1 + x_2}{k} + \frac{x_1x_2}{k}}}^k \\
    =& \lim_{k \rightarrow \infty} \left[1 + k\bkt{\frac{x_1 + x_2}{k} + \frac{x_1x_2}{k^2}}\right. \tag{binomial theorem}\\
    &+ \frac{k(k-1)}{2}\bkt{\frac{x_1 + x_2}{k} + \frac{x_1x_2}{k^2}}^2 + \cdots
\end{align*}

In the limit, all the terms that have a factor of $\frac{1}{k^2}$ will go to $0$,
\[
    = 1 + (x_1 + x_2) + \frac{1}{2}(x_1 + x_2)^2 + \cdots
\]

%%%%%%%%%%%%%%%%%%%%%%
% Lecture 6: 18/10/2023
%%%%%%%%%%%%%%%%%%%%%%

The exponential property allows us to write
\[
    \exp(x) = e^x
\]
where $e = \exp(1) = \lim_{k \rightarrow \infty}(1+\frac{1}{k})^k = 2.71828\ldots$ Note that this definition allows us to extend the idea of exponentiation beyond the integers and rationals. 
For instance, we define $e^\pi$ as the $\exp(\pi)$. We also have the inverse function of the exponentials, $\ln(x)$ whereby
\[
  \exp(\ln(x)) = x  
\]
It follows from this that
\[
    a^x = e^{x \ln a} = \exp(x \ln a) 
\]
This allows us to differentiate,
\[
    \diff{}{x}a^x = a^x \ln a
\]
We discuss the exponential function in this context as we can think of it as an eigen function to the differential operator.

\begin{defi}[Eigen Function]
    An \emph{Eigen Function} of the derivative operator is a function that is unchanged, up to a multiplicative scaling (\emph{eigen value}) under the action of the differential operator. If $f$ is an eigen function,
    \[
        \diff{f}{x} = \lambda f(x)  
    \]
    The eigen functions of $\diff{}{x}$ are $e^\lambda x$ with eigen value $\lambda$.
\end{defi}

\subsection{First-order linear ODE}
\subsubsection{Homogeneous first order linear ODEs}
We will begin by looking at very specific cases of ODEs, let's define some of the terms in the title
\begin{itemize}
    \item Linear: the dependant variable ($y$) and its derivative only appear linearly
    \item First order: the highest derivative that appears in the first derivative
    \item Homogeneous: an ODE in which all terms involves the dependant variable or its derivatives. For all of these $y=0$ is a trivial solution
\end{itemize}
Furthermore, we will further specialise to the simple case of constant coefficients,\
\begin{itemize}
    \item Constant coefficients: the indepdnant variable ($x$) does not appear explicitly.
\end{itemize}

\begin{eg}
    \[
        5 \diff{y}{x} - 3y = 0  
    \]
    We will try $y = Ae^{\lambda x}$, we have $\diff{y}{x} = \lambda A e^{\lambda x}$. This gives
    \begin{align*}
        5\lambda A e^{\lambda x} - 3Ae^{\lambda x} = 0 \\
        \Rightarrow 5\lambda - 3 = 0
    \end{align*}
    We call this the characteristic equation. Solving this gives,
    \[
        \lambda = \frac{3}{5} \Rightarrow y = Ae^{\frac{3}{5} x}   
    \]
    Since this equation is linear and homogenenous, this solution holds for all $A$.
\end{eg}

\begin{remark}\leavevmode
    \begin{itemize}
        \item Generally, for any linear homogenenous ODE any constant multiple of a solution is also a solution.
        \item An $n^{th}$ order linear ODE will have exactl $n$ indepdnant solutions
    \end{itemize}
    In the example above, $y = Ae^{\frac{3}{5} x}$ is the general solution. To specify a unique solution, requires giving suitable boundary conditions; eg. $y(0)$, and in general $n$ conditions will be required for $n^{th}$ order ODEs.
\end{remark}

\subsubsection{Discrete Equations}

Consider the same equation from above,
\[
    5 \diff{y}{x} - 3y = 0 \quad \text{with} y(0) = y_0
\]
We can try to solve it neumerically. We will approximate the equation by using the discrete form of it over the point $\{x_n\}$ where $x_n = nh$. For some step $h$, we approximate the derivative to be
\[
    \diff{y}{x}\Big{|}_{x_n} \approx \frac{y_{n+1} - y_n}{h}    
\]
This is known as the ``forward Euler scheme'', which in practice is not that great of an approximation. Applying the approximation we get
\begin{align*}
    \frac{5}{h}(y_{n+1} - y_n) - 3y_n = 0 \\
    \Rightarrow y_{n+1} = (1 + \frac{3h}{5})y_{n} \\
    \therefore y_n = (1 + \frac{3h}{5})^ny_0 = (1 + \frac{3x_n}{5n})^ny_0 \tag{$x_n = nh$}
\end{align*}

We would like to check that this approximation agrees with what we solved for earlier. If we take $x_n = x$ and take $n$ steps between $x=0$ and $x$, considering the limit as $n \rightarrow \infty$,
\[
    \lim_{n \rightarrow \infty}y_n = \lim_{n \rightarrow \infty} (1 + \frac{3x_n}{5n})^ny_0 = e^{\frac{3x}{5}}y_0
\]

In general, difference equations of this form tend to be related to first order ODEs where the solution ends up being some exponential form.

\subsubsection{Series Solutions}

We can also try to look for solutions as power series of the form:
\[
    y(x) = \sum_{n=0}^{\infty}{a_nx^n}    
\]
where we will try to determine the coefficients $a_n$ by substituting into the ODE. Many, but not all ODEs have solutions of this form.

We will consider the same equation from above,
\[
    5 \diff{y}{x} - 3y = 0
\]
Using $y$ of the from above, we have
\begin{align*}
    \diff{y}{x} = \sum_{n=0}^{\infty}{na_nx^{n-1}} \\
    \Rightarrow x\diff{y}{x} = \sum_{n=1}^{\infty}{na_nx^{n}}
\end{align*}
Also we can come with an expression for $xy$
\[
    xy = \sum_{n=0}^{\infty}{a_nx^{n+1}} = \sum_{n=1}^{\infty}{a_{n-1}x^{n}}
\]
we have done all this manipulation to get the sum indices to line up nicely. Substituting into the equation we get
\[
    \sum_{n=1}^{\infty}(5na_n - 3a_{n-1})x^n = 0
\]
In order for the sum to be $0$ for all $x$, we need $(5na_n - 3a_{n-1}) = 0$
\[
    \Rightarrow a_n = \frac{3}{5n}a_{n-1} \quad (n \geq 1)
\]
It follows that,
\begin{align*}
    a_n = \bkt{\frac{3}{5n}}\bkt{\frac{3}{5(n-1)}}\bkt{\frac{3}{5(n-2)}}\cdots = \bkt{\frac{3}{5}}^n\frac{1}{n!}a_0 \\
    \therefore y(x) = a_0 \sum_{n=0}^{\infty}\bkt{\frac{3x}{5}}^n\frac{1}{n!} \\
    = a_o e^\frac{3x}{5}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%
% Lecture 7: 20/10/2023
%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Forced (inhomogeneous) ODEs}

\end{document}