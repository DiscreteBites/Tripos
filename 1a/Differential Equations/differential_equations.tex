\documentclass{article}

\def\npart {IA}
\def\nterm {Michaelmas}
\def\nyear {2023}
\def\nlecturer {Prof A.\ D.\ Challinor}
\def\ncourse {Differential Equations}

\input{../../header.tex}

\begin{document}
\maketitle{
    \small
    \noindent\textbf{Basic calculus}\\
    Informal treatment of differentiation as a limit, the chain rule, Leibnitz's rule, Taylor series, informal treatment of $O$ and $o$ notation and l'H\^opital's rule; integration as an area, fundamental theorem of calculus, integration by substitution and parts.\hspace*{\fill}[3]
  
    \vspace{5pt}
    \noindent Informal treatment of partial derivatives, geometrical interpretation, statement (only) of symmetry of mixed partial derivatives, chain rule, implicit differentiation. Informal treatment of differentials, including exact differentials. Differentiation of an integral with respect to a parameter.\hspace*{\fill}[2]
  
    \vspace{10pt}
    \noindent\textbf{First-order linear differential equations}\\
    Equations with constant coefficients: exponential growth, comparison with discrete equations, series solution; modelling examples including radioactive decay.
  
    \vspace{5pt}
    \noindent Equations with non-constant coefficients: solution by integrating factor.\hspace*{\fill}[2]
  
    \vspace{10pt}
    \noindent\textbf{Nonlinear first-order equations}\\
    Separable equations. Exact equations. Sketching solution trajectories. Equilibrium solutions, stability by perturbation; examples, including logistic equation and chemical kinetics. Discrete equations: equilibrium solutions, stability; examples including the logistic map.\hspace*{\fill}[4]
  
    \vspace{10pt}
    \noindent\textbf{Higher-order linear differential equations}\\
    Complementary function and particular integral, linear independence, Wronskian (for second-order equations), Abel's theorem. Equations with constant coefficients and examples including radioactive sequences, comparison in simple cases with difference equations, reduction of order, resonance, transients, damping. Homogeneous equations. Response to step and impulse function inputs; introduction to the notions of the Heaviside step-function and the Dirac delta-function. Series solutions including statement only of the need for the logarithmic solution.\hspace*{\fill}[8]
  
    \vspace{10pt}
    \noindent\textbf{Multivariate functions: applications}\\
    Directional derivatives and the gradient vector. Statement of Taylor series for functions on $\R^n$. Local extrema of real functions, classification using the Hessian matrix. Coupled first order systems: equivalence to single higher order equations; solution by matrix methods. Non-degenerate phase portraits local to equilibrium points; stability.
    
    \vspace{5pt}
    \noindent Simple examples of first- and second-order partial differential equations, solution of the wave equation in the form $f(x + ct) + g(x - ct)$.\hspace*{\fill}[5]}

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%
% Lecture 1: 6/10/2023
%%%%%%%%%%%%%%%%%%%%%%
\section{Basic Calculus}
\subsection{Differentiation}
\begin{defi}[Derivative of a function]
    Define the derivative of $f(x)$ w.r.t its argument $x$ as the function
    \[
        \diff{f}{x} \equiv \lim_{h \rightarrow 0}{\frac{f(x +h) - f(x)}{h}}
    \]

    % Insert the diagram of the slopy thing.

    Note that for a derivative to exist at point $x$, both the left and right hand limits must exist and be equal. 
    \[
        \lim_{h \rightarrow 0^-}{\frac{f(x +h) - f(x)}{h}} = \lim_{h \rightarrow 0^+}{\frac{f(x +h) - f(x)}{h}}
    \]
\end{defi}

\begin{eg}[$f(x) = \abs{x}$]
    \begin{align*}
        \text{LHL} &= \lim_{h \rightarrow 0^-}{\frac{\abs{h}}{h}} = -1 \\
        \text{RHL} &= \lim_{h \rightarrow 0^-}{\frac{\abs{h}}{h}} = 1
    \end{align*}
    We see that $\abs{x}$ is not differentiable at $x=0$
\end{eg}

Some notational differences. Most often Newton's notation is used to denote differentiation with respect to time. 
\begin{center}
    \begin{tabular}{ c|c|c }    
        Leibnitz & Lagrange & Newton \\
        \midrule
        $\diff{f}{x}$ & $f^{\prime}(x)$ & $\dot{f}(x)$
    \end{tabular}
\end{center}

For sufficiently smooth functions, we can define higher derivatives. Here are some second derivatives
\[
    \diff{}{x}\left(\diff{f}{x}\right) = \diff[2]{f}{x} \quad \text{or} \quad f^{\prime\prime}(x) \quad \text{or} \quad \ddot{f}(x)
\]
For $n^{th}$ derivatives we tend to use this notation
\[
    f^{(n)}(x)
\]

\subsection{Big $O$ and little $\underline{o}$ notation}.
These are known as order parameters and are very useful when comparing the behaviour of 2 functions sufficiently close to some point $x_o$ or as $x$ tends to infinity. They are particular useful in deal with approximations where the remainder depends upon a parameter.

\begin{defi}[$O$ and $\underline{o}$]\leavevmode
    \begin{enumerate}
        \item $f(x)$ is $O(g(x))$ as $x \rightarrow x_0$ if $\exists \delta > 0$ and $M > 0$ such that $\forall x$ with $0 < \abs{x - x_0} < \delta$,
        \[
            \abs{f(x)} \leq M\abs{g(x)}  
        \]
        It follows from this that $\frac{f(x)}{g(x)}$ remains bounded as $x \rightarrow x_0$
        \item $f(x)$ is $\underline{o}(g(x))$ as $x \rightarrow x_0$ if $\forall \epsilon > 0, \exists \delta > 0$ such that $\forall x$ with $0 < \abs{x - x_0} < \delta$,
        \[
            \abs{f(x)} \leq \epsilon\abs{g(x)}  
        \]
        This means that $f(x)$ needs to be much smaller than $g(x)$. It follows similarly that if $g(x) \neq 0 $
        \[
            \lim_{x \rightarrow x_0}{\frac{f(x)}{g(x)}} = 0  
        \]
    \end{enumerate}
    We often write that $f(x) = O(g(x))$ or $f(x) = \underline{o}(g(x))$, but this is an abuse of notation. A more accurate notation would be to write
    \[
        f(x) \in O(g(x)) \quad f(x) \in \underline{o}(g(x))
    \]
    Since these actually represent classes of functions. We can also notice that $\underline{o}$ is a much stronger statement than $O$ since, for $\underline{o}$ we need to be able to produce a value of $x$ that results in an arbitrarily small multiple of $g(x)$

    We can also extend this to behaviour as $x \rightarrow \infty$. We say that $f(x) = O(g(x))$ if $\exists X > 0$ and $M > 0$ such that $\forall x > X$,
    \[
        \abs{f(x)} \leq M\abs{g(x)}  
    \]

    Note that
    \[
        f(x) = \underline{0}(g(x)) \Rightarrow f(x) = O(g(x))  
    \]
\end{defi}

\begin{eg}\leavevmode
    \begin{enumerate}
        \item Given $f(x) = 2x, \quad f(x) = O(x)$ as $x \rightarrow 0$. However, $f(x) \neq \underline{o}(g(x))$ as $x \rightarrow 0$ since,
        \[
            \lim_{x \rightarrow 0}{\abs{\frac{f(x)}{x}}} = 2 \neq 0  
        \]
        \item Given $f(x) = x^2, \quad f(x) = \underline{o}(x)$ as $x \rightarrow 0$ since, 
        \[
            \lim_{x \rightarrow 0}{\abs{\frac{x^2}{x}}} = 0
        \]
        \item Given $f(x) = x^2 + x, \quad f(x) = O(x^2)$ as $x \rightarrow \infty$ since for any $x > 1$,
        \[
            \abs{x^2 + x} \leq 2\abs{x^2}  
        \]
        In general, any polynomial where $a_n \neq 0$
        \[
            a_nx^n + a_{n-1}x^{n-1} + \cdots + a_1x + a_0 = O(x^n) \quad \text{as} \quad x \rightarrow 0
        \]
        \item Given $f(x) = \sin{2x}, \quad f(x) = O(x)$ as $x \rightarrow 0$ since, $\sin{2x} \approx 2x$ for $x << 1$
    \end{enumerate}
\end{eg}

\begin{remark}
    When using order paramters, constants do not matter. If 
    \begin{align*}
        f(x) &= O(g(x)) \Rightarrow af(x) = O(g(x)) \\
        f(x) &= O(ag(x)) \quad \text{for} \ a \neq 0
    \end{align*}
\end{remark}

Order paramters are useful to classify remainder terms before taking a limit. Consider the following expression
\[
    f(x_0 + h) - f(x_0) = h f^{\prime}(x_0) + \varepsilon(h) 
\]
By comparing this with the definition of the derivative, we can take a limit
\begin{align*}
    \lim_{h \rightarrow 0}{\frac{f(x_0 + h) - f(x_0)}{h}} &= f^{\prime}(x_0) + \lim_{h \rightarrow 0}{\varepsilon(h)} \\
    &\therefore  \lim_{h \rightarrow 0}{\varepsilon(h)} = 0 \\
    &\Rightarrow \varepsilon(h) = \underline{o}(h)
\end{align*}
\[
    \therefore f(x_0 + h) = f(x_0) + h f^{\prime}(x_0) + \underline{o}(h)
\]

%%%%%%%%%%%%%%%%%%%%%%
% Lecture 2: 8/10/2023
%%%%%%%%%%%%%%%%%%%%%%

\subsection{Rules for differentiation}
\begin{thm}[Chain Rule]
    Given $f(x) = F(g(x))$,
    \[
        \diff{f}{x} = F^{\prime}(g(x)) \cdot \diff{g}{x}   = \diff{F}{g}\diff{g}{x}
    \]
    Note that $F^{\prime}(g(x))$ refers to the derivative of the function $F$ with respect to its argument $g$ and then evaluated at $g(x)$
\end{thm}

\begin{proof}[See notes]
    
\end{proof}

\begin{eg}
    \[
        \diff{}{x} \sin{(x^2 -x +2)} = \cos{(x^2 -x +2)}[2x - 1]
    \]
\end{eg}

\begin{thm}[Product Rule]
    Given $f(x) = u(x)v(x)$,
    \[
        \diff{f}{x} = v \diff{u}{x} + u \frac{dv}{dx}
    \]
    The Quotient rule is a special case of the product rule, when $v \rightarrow \frac{1}{v}$
    \begin{align*}
        \diff{}{x}\left(\frac{u}{v}\right) &= \frac{1}{v}\diff{u}{x} + u \diff{}{x}\left(\frac{1}{v}\right) \\
        &= \frac{1}{v}\diff{u}{x} + \frac{u}{v^2} \frac{dv}{dx} \\
        &= \frac{vu^{\prime} - v^{\prime}u}{v^2}
    \end{align*}
\end{thm}

\begin{thm}[Leibnitz'z Rule]
    Consider $f(x) = u(x)v(x)$ then,
    \begin{align*}
        f^{\prime} &= u^{\prime}v + uv^{\prime} \\
        f^{\prime\prime} &= u^{\prime\prime}v + 2u^{\prime}v^{\prime} + uv^{\prime\prime} \\
        f^{\prime\prime\prime} &= u^{\prime\prime\prime}v + 3u^{\prime\prime}v^{\prime} + 3u^{\prime}v^{\prime\prime} + uv^{\prime\prime\prime} \\
        \vdots
    \end{align*}

    We have that
    \[
        f^{(n)}(x) = \sum_{r=0}^{n}{\binom{n}{k}u^{(n-r)}(x)v^{r}(x)}
    \]
    Note that
    \[
        u^{(0)}(x) \equiv u(x)  
    \]
\end{thm}

\begin{proof}[Exercise by induction]
    
\end{proof}

\subsection{Taylor Series}
A taylor series is an infinite series that tries to approximate a function in the vincity of some point. It is constructed to match the first $n$ differentials of a function.

\begin{defi}[Taylor Series]
    For $f(x)$ infinitely differentiable at $x_0$, the taylor series $T_f(x)$ is defined to be
    \[
        T_f(x) = f(x_0) + (x - x_0)f^{\prime}(x) + \frac{(x - x_0)^2}{2!}f^{\prime\prime}(x) + \cdots + \frac{(x - x_0)^n}{n!}f^{(n)}(x) + \cdots
    \]
\end{defi}

Taylor series are often used for approximations. To consider the errors these approximations have, we look at Taylor's Theorem.

\begin{thm}[Taylor's Theorem]
    For $n$-times differentiable $f(x)$ at $x_0$,
    \[
        f(x_0 + h) = f(x_0) + hf^{\prime}(x_0) + \frac{h^2}{2!}f^{\prime\prime}(x_0) + \cdots + \frac{h^n}{n!}f^{(n)}(x) + E_n
    \]
    Where $E_n$ accounts for the error in the approximation. Taylor's theorem states that
    \[
        E_n = \underline{o}(h^n) \quad \text{as} \quad x \rightarrow 0  
    \]
\end{thm}

A stronger form of this exists, if $f^{(n+1)}(x)$ exists $\forall x \in [x_0, x_0 + h]$ as in continuous in this range,
\begin{align*}
    E_n &= O(h^{n+1}) \quad \text{as} \quad x \rightarrow 0 \\
    &= \frac{f^{n+1}(x_n)}{(n+1)!}h^{n+1} \quad \text{for} \quad x \in [x_0, x_0 +h]
\end{align*}
This is due to Lagrange. Note that $E_n = O(h^{n+1})$ is stronger than $\underline{o}(h^n)$. For instance, consider $h^{n + 0.5} = \underline{o}(h^n)$ but $\neq O(h^{n+1})$ as $h \rightarrow 0$

\begin{defi}[Taylor Polynomials]
    With $x = x_0 + h$ and Taylor's theorem gives
    \[
        f(x) = f(x_0) + (x - x_0)f^{\prime}(x) + \frac{(x - x_0)^2}{2!}f^{\prime\prime}(x) + \cdots + \frac{(x - x_0)^n}{n!}f^{(n)}(x) + E_n
    \]
    This is called the ``$n^{th}$'' Taylor polynomial about $x_0$. It is constructed to match the first $n$ derivatives of $f(x)$ as $x_0$ and provides a local approximation to $f(x)$ in the vicinity of $x_0$ with error $E_n = O(h^{n+1})$. If we have that
    \[
        \lim_{n \rightarrow \infty}{E_n} = 0  
    \]
    then the Taylor Polynomial converges to $f(x)$
\end{defi}

\subsection{L'H\^opital's Rule}
This theorem allows us to deal with limits of \emph{indeterminant forms}. For example $\lim_{x \rightarrow x_0}{\frac{f(x)}{g(x)}}$ where $\lim_{x \rightarrow x_0}{f(x)} = \lim_{x \rightarrow x_0}{g(x)} = 0$.

\begin{thm}[L'H\^opital's Rule]
    Let $f(x)$ and $g(x)$ be differentiable at $x_0$ and have continuous first derivatives there, and
    \begin{align*}
        \lim_{x \rightarrow x_0}{f(x)} &= f(x_0) = 0 \\
        \lim_{x \rightarrow x_0}{g(x)} &= g(x_0) = 0
    \end{align*}
    Then if $g^{\prime}(x_0) \neq 0$,
    \[
        \lim_{x \rightarrow x_0}{\frac{f(x)}{g(x)}} = \lim_{x \rightarrow x_0}{\frac{f^{\prime}(x)}{g^{\prime}(x)}}
    \]
    Provided that the second limit exists. Note that the non-existence of the second limit does not give any information about the original limit.
\end{thm}

\begin{proof}
    Using the result at the end of the section discussing order parameters,
    \begin{align*}
        f(x) &= f(x_0) + (x - x_0)f^{\prime}(x_0) + \underline{o}(x - x_0) \quad \text{as} \quad x \rightarrow x_0 \\
        g(x) &= g(x_0) + (x - x_0)g^{\prime}(x_0) + \underline{o}(x - x_0) \quad \text{as} \quad x \rightarrow x_0
    \end{align*}
    \[
        \lim_{x \rightarrow x_0}{\frac{f(x)}{g(x)}} = \lim_{x \rightarrow x_0}{\frac{f^{\prime}(x_0) + \frac{\underline{o}(x - x_0)}{x - x_0}}{g^{\prime}(x_0) + \frac{\underline{o}(x - x_0)}{x - x_0}}}
    \]
    Since we have that $g^{\prime}(x_0) \neq 0$, 
    \begin{align*}
        \lim_{x \rightarrow x_0}{\frac{f(x)}{g(x)}} &= \frac{f^{\prime}(x_0)}{g^{\prime}(x_0)} \\
        &= \frac{\lim_{x \rightarrow x_0}{f^{\prime}(x)}}{\lim_{x \rightarrow x_0}{g^{\prime}(x)}} \tag{assuming continuous first derivatives}\\
        &= \lim_{x \rightarrow x_0}{\frac{f^{\prime}(x)}{g^{\prime}(x)}} \tag{already checked $g(x_0) \neq 0$}
    \end{align*}
\end{proof}

We can generalise, by repeating L'\^opital's Rule, given that the conditions hold. For instance, we can have if $f^{\prime}(x_0) = g^{\prime}(x_0) = 0$
\[
    \lim_{x \rightarrow x_0}{\frac{f(x)}{g(x)}} = \frac{f^{\prime\prime}(x_0)}{g^{\prime\prime}(x_0)}
\]

\begin{eg}
    \begin{align*}
        f(x) &= 3\sin{x} - \sin{3x}
        g(x) = 2x - \sin{2x}
    \end{align*}
\end{eg}

%%%%%%%%%%%%%%%%%%%%%%
% Lecture 3: 11/10/2023
%%%%%%%%%%%%%%%%%%%%%%

\section{Integration}
\subsection{Reimann sums}
We will formalise the idea of taking the area under a function. 

\begin{defi}[Integral]
    The integral of a suitably well-defined function $f(x)$ is the limit of a sum
    \begin{align*}
        \int_{a}^{b}{f(x) \ \diffd x} &\equiv \lim_{N \rightarrow \infty}{\sum_{n=0}^{N-1}{f(x_n)\Delta x}} \\
        &= \lim_{\Delta x \rightarrow 0}{\sum_{n=0}^{N-1}{f(x_n)\Delta x}}
    \end{align*}
    Where $\Delta x = \frac{b - a}{N}$ and $x_n = a + n\Delta x$
    
    % Insert diagram of reimann sums
    Note that the $\lim_{N \rightarrow \infty}$ is independant on how the rectangles are drawn. For instance, we can have $\Delta x$ be non-uniform. (see Analysis I)
\end{defi}

As we have drawn, it appears that each partial sum will underestimate the limit of the area.
\begin{prop}
    We will show that in the limit as the $\Delta x \rightarrow 0$, that the difference between the area and the limit approaches $0$.
\end{prop}
\begin{proof}
    Consider one rectangle for finite $N$. 
    % insert diagram
    Using the mean value theorem, there exists a $c$ in the range $x_n \leq c \leq x_{n+1}$ such that the area $A_n$
    \[
        A_n = (x_{n+1} - x_n)f(c)  
    \]
    If $f(x)$ is differentiable we have from Taylor's theorem that
    \[
        f(c) = f(x_n) + O(c - x_n), \quad \text{as} \ c-x_n \rightarrow 0  
    \]
    Since $\Delta x$ is larger than $c - x_n$,
    \[
        f(c) = f(x_n) + O(\Delta x)
    \]
    Thus it follows that
    \[
        A_n =  \Delta x f(x_n) + \Delta x O(\Delta x), \quad \text{as} \ \Delta x \rightarrow 0 
    \]
    Note that
    \[
        \Delta x O(\Delta x) = O(\Delta x^2)
    \]
    The total area between $x=a$ and $x=b$ is therefore,
    \[
        A = \lim_{N \rightarrow \infty}{\sum_{n=0}^{N-1}{A_n}} = \lim_{N \rightarrow \infty}{\sum_{n=0}^{N-1}{\Delta x f(\Delta x)}} + \lim_{N \rightarrow \infty}{\sum_{n=0}^{N-1}{O(\Delta x^2)}}
    \]
    Note that
    \[
        \lim_{N \rightarrow \infty}{\sum_{n=0}^{N-1}{\Delta x f(\Delta x)}}
    \]
    Is the definition of the reimann sum and therefore the error is given by
    \begin{align*}
        \lim_{N \rightarrow \infty}{\sum_{n=0}^{N-1}{O(\Delta x^2)}} &= \lim_{N \rightarrow \infty}{NO \bkt{\bkt{\frac{b-a}{N}}^2}} \\
        &= \lim_{N \rightarrow \infty}{O\bkt{\frac{(b-a)^2}{N}}} \\
        &= 0
    \end{align*}
\end{proof}

\subsection{Fundamental Theorem of Calculus}
We will formalise the idea of taking the inverse of differentiation.

\begin{thm}[FTC]
    Let $F(x)$ be defined as
    \[
        F(x) = \int_{a}^{x}{f(t) \ \diffd t}  
    \]
    Then,
    \[
        \diff{F}{x} = f(x)
    \]
\end{thm}
\begin{proof}
    \begin{align*}
        \diff{F}{x} &= \lim_{h \rightarrow 0}\frac{F(x + h) - F(x)}{h} \\
        &= \lim_{h \rightarrow 0}\frac{1}{n}\sqbkt{\int_{x}^{x+h}{f(t) \ \diffd t}} \\
        &= \lim_{h \rightarrow 0}\frac{1}{n}\sqbkt{f(x)h + O(h^2)} \tag{from reimann sum definition} \\
        &= f(x) + \lim_{h \rightarrow 0}O(h^2) \\
        &= f(x)
    \end{align*}
\end{proof}

\begin{remark}
    $F(x)$ is a solution of the differential equation $\diff{F}{x} = f(x)$ with the added property that $F(a) = 0$
\end{remark}

\begin{cor}
\[
    \diff{}{x}\int_{x}^{b}{f(x) \ \diffd t} = -f(x)
\]
and by the chain rule, 
\[
    \diff{}{x}\int_{a}^{g(x)}{f(x) \ \diffd t} = f(g(x))g'(x)
\]
\end{cor}

\begin{notation}
    We typically write indefinite integrals as $\int{f(x) \ \diffd x}$. We may also write
    \[
        \int^{x}{f(t) \ \diffd t}    
    \]
    wich is arguably better as it makes clear that the result is a function of $x$. Further note that the undefined lower limit leaves space for the constant of integration $c$.
\end{notation}

\subsection{Methods of Integration}

\begin{defi}[Substitution]
    Substitution is useful if the integrand contains composed functions, or if we can spot the derivative of a function and itself.
\end{defi}
\begin{eg}
    \[
        I = \int{\frac{1 - 2x}{\sqrt{x - x^2}} \ \diffd x}
    \]
    Let $u = x - x^2$, $\diffd u = (1 - 2x) \diffd x$
    \begin{align*}
        I &= \int{\frac{1}{\sqrt{u}} \ \diffd u} \\
        &= 2\sqrt{u} \\
        &= 2\sqrt{x - x^2} + x
    \end{align*}
\end{eg}
Here are some useful substitution to consider, motivated by the trigonometric and hyperbolic identities
\begin{center}
    \begin{tabular}{c | c}
        Term in integrand & Substitution \\
        \midrule
        $1 - x^2$ & $x = \sin{\theta} or \tanh{\theta}$\\
        $1 + x^2$ & $x = \tan{\theta} or \sinh{\theta}$\\
        $x^2 - 1$ & $x = \sec{\theta} or \cosh{\theta}$\\

    \end{tabular}
\end{center}

\begin{eg}
    \[
        I = \int{\sqrt{x - x^2} \ \diffd x} = \int{\sqrt{1 - (x - 1)^2} \ \diffd x}    
    \]
    Let $x - 1 = \sin{\theta}$, $\diffd x = \cos{\theta} \diffd \theta$
    \begin{align*}
        I &= \int{\cos^2{\theta} \ \diffd \theta} \\
        &= \int{\frac{\cos{2\theta} + 1}{2}} \ diffd \theta \\
        &= \frac{1}{4}\sin{2\theta} + \frac{1}{2}\theta + c \\
        &= \frac{1}{2}\sin^{-1}{(x-1)} + \frac{1}{2}(x-1)\sqrt{2x - x^2} + c
    \end{align*}
\end{eg}

\begin{defi}[By parts]
    Integration by parts follows from the product rule. Recall
    \[
        (uv)' = uv' + u'v
    \]
    We can then rearrange
    \[
        uv' = (uv)' - u'v \Rightarrow \int{uv' \diffd x} = uv - \int{u'v \diffd x}
    \]
\end{defi}

\begin{eg}\leavevmode
    \begin{enumerate}
        \item \begin{align*}
            I = \int_{0}^{\infty}{xe^{-x} \diffd x} &= \left. -xe^{-x} \right\rvert_{0}^{\infty} + \int_{0}^{\infty}{e^-x} \\
            &= 0 + 1 = 1
        \end{align*}
        \item \[
            I = \int{\ln{x} \diffd x} = x\ln{x} - x + c    
        \]
    \end{enumerate}
\end{eg}

%%%%%%%%%%%%%%%%%%%%%%
% Lecture 4: 13/10/2023
%%%%%%%%%%%%%%%%%%%%%%

\begin{remark}
    We will briefly give an outline of the proof of taylor's theorem
\end{remark}
\begin{proof}
    Using FTC, we can expand
    \[
        f(x) = f(0) + \int_{0}^{x}{f'(t) \diffd t}
    \]
    Then, we insert a factor of $1 = -\frac{d(x-t)}{dt}$ and integrate by parts,
    \begin{align*}
        f(x) &= f(0) + \int_{0}^{x}{f'(t) \diffd t} \\
        &= f(0) + \int_{0}^{x}{-\frac{d(x-t)}{dt} \times f'(t) \diffd t} \\
        &= f(0) + \int_{0}^{x}{-\frac{d(x-t)}{dt} \times f'(t) \diffd t} \\
        &= f(0) - (x-t)f'(t) \Big{|}_{0}^{x} - \int_{0}^{x}{-(x-t) \times f''(t) \diffd t} \\
        &= f(0) + xf'(0) + \int_{0}^{x}{(x-t) \times f''(t) \diffd t}
    \end{align*}
    We can continue this expansion, replacing $(x - t) = -\frac{1}{2}\diff{}{t}(x-t)^2$
    \begin{align*}
        f(x) &= f(0) + xf'(0) + \int_{0}^{x}{(x-t) \times f''(t) \diffd t} \\
        &= f(0) + xf'(0) + \int_{0}^{x}{-\frac{1}{2}\diff{(x-t)^2}{t} \times f''(t) \diffd t} \\
        &= f(0) + xf'(0) - \frac{1}{2}(x-t)^2 f''(t) \Big{|}_{0}^{x} - \int_{0}^{x}{-\frac{1}{2}(x-t)^2 \times f'''(t) \diffd t} \\
        &= f(0) + xf'(0) + \frac{1}{2}x^2 f''(0) + \frac{1}{2}\int_{0}^{x}{(x-t)^2 \times f'''(t) \diffd t}
    \end{align*}
    In general, we will be replacing terms of the form $(x - t)^n$ with $-\frac{1}{n+1} \diff{}{t}(x - t)^{n+1}$. Doing so with return the first $n$ terms of the taylor expansion,
    \[
    f(x) = f(0) + xf'(0) + \frac{x^2}{2} f''(0) + \ldots + \frac{x^n}{n!}f^{(n)}(0) + \frac{1}{n!} \int_{0}^{2}{(x-t)^n f^{(n+1)}(t) \diffd t}
    \]
    Therefore, we recover an integral form of the error term
    \[
        E_n = \frac{1}{n!} \int_{0}^{2}{(x-t)^n f^{(n+1)}(t) \diffd t}
    \]
    Using the mean value theorem, we can find a value $x_n$ in the interval that satisfies the intergral
    \[
        E_n = \frac{f^{(n+1)}(x_n)}{n!} \int_{0}^{2}{(x-t)^n \diffd t} = \frac{f^{(n+1)}(x_n)}{(n+1)!} x^{n+1}
    \]
    where $0 \leq x_n \leq x$
\end{proof}

\section{Partial Differentiation}
\subsection{Functions of serveral variables}
In this chapter, we will generalise differentiation to function of more than one variable. For instance, we may consider the height of terrain as a function of longitude and latitude, or the density of air as a function of position and time. 
Consider the first example, in the diagram we plot the contours of the function on the $x-y$ plane (the curves for which the height is constant).
% insert diagram

Notice that the slop of the function at some point $A$ depends on the direction. Therefore, we will find the slope of the graph along the $x$ and $y$ direction independantly, where we will be able to combine the them to find the slopes in any direction.
\subsection{Partial derivatives}
\begin{defi}[Partial derivative]
    Given a function of several variables, $f(x, y)$ we define the \emph{partial derivative} of $f$ w.r.t. $x$ at fixed $y$ given by
    \[
        \diffp{f}{x} \Big{|}_{y} = \lim_{\delta x \rightarrow 0}{\frac{f(x + \delta x, y) - f(x, y)}{\delta x}} 
    \]
    This limit therefore find the slope of $f$ when moving in the positive $x$ direction. Similarly, we may define
    \[
        \diffp{f}{y} \Big{|}_{x} = \lim_{\delta y \rightarrow 0}{\frac{f(x, y + \delta y) - f(x, y)}{\delta y}} 
    \]
\end{defi}

\begin{remark}
    For suitably smooth functions we can also define higher derivates,
    \[
        \diffp{f}{{x^2}} = \diffp{}{x}\bkt{\diffp{f}{x}} \quad \ldots \quad \diffp[n]{f}{x} = \diffp{}{x}\bkt{\diffp[n-1]{f}{x}}
    \]
    as well as mixed partial derivatives,
    \[
        \diffp{}{y}\bkt{\diffp{f}{x}} = \diffp{f}{{y}{x}}
    \]
    where the order of the partial derivatives taken starts from the right.
\end{remark}

\begin{eg}[$f(x) = x^2 + y^2 + e^{xy^2}$]
    We can compute first derivatives
    \[
        \diffp{f}{x} \Big{|}_{y} = 2x + y^2e^{xy^2} \qquad \diffp{f}{y} \Big{|}_{x} = 2y + 2xye^{xy^2}
    \]
    and similarly for second derivatives
    \[
        \diffp[2]{f}{x} \Big{|}_{y} = 2 + y^4e^{xy^2} \qquad \diffp[2]{f}{y} \Big{|}_{x} = 2 + 2xe^{xy^2} + (2xy)^2e^{xy^2}
    \]
    as well as the mixed partial derivatives
    \begin{align*}
        \diffp{}{x}\bkt{\diffp{f}{y} \Big{|}_{x}} \Big{|}_{y} &= 2ye^{xy^2} + 2xy^3e^{xy^2} \\
        \diffp{}{y}\bkt{\diffp{f}{x} \Big{|}_{y}} \Big{|}_{x} &= 2ye^{xy^2} + 2xy^3e^{xy^2} \\
    \end{align*}
    Note that the mixed partial derivates are equaiavalent. In cases where the function has second partial derivatives, mixed second partial derviatives will be the same. This is known as Schwarz's Theorem.
\end{eg}

\begin{notation}
    As a useful convension, we will omit the $\Big{|}_{x}$, and work on the assumption that all other variables are held constant. There are also some alternative forms,
    \[
        f_x = \diffp{f}{x} \qquad f_{xy} = \diffp{f}{{y}{x}}  
    \]
\end{notation}
\subsection{Multivariate chain rule}
As we have the chain rule for regular differentiation, we are interested in extending this to partial derivatives. Consider the following question. Suppose I am walking around some terrain. What is the derivative of my height with respect to time.

Given a path $x(t), y(t)$ and some function $f(x(t), y(t))$, what is $\diff{f}{t}$? We consider a change in $f$ when we move from 
\[
    (x, y) \mapsto (x + \delta x, y + \delta y)    
\]
We can expand out, and include a term that "bridges" the gap
\begin{align*}
    \delta f = f(x + \delta x, y + \delta y) & &- f(x, y) \\
    = f(x + \delta x, y + \delta y) &- f(x + \delta x, y) \\
    &+f(x + \delta x, y) &- f(x, y)
\end{align*}
Applying Taylors expansion we have,
\[
    f(x + \delta x, y) - f(x, y) = f_x(x, y)\delta x + \ltlo{\delta x}
\]
as well as,
\[
    f(x + \delta x, y + \delta y) - f(x + \delta x, y) = f_y(x + \delta x, y)\delta y + \ltlo{\delta y}
\]
where we can further expand
\[
    f_y(x + \delta x, y) = f_y(x, y) + f_{yx}(x, y)\delta x + \ltlo{\delta x}
\]
Together,
\[
    \delta f = f_x(x, y)\delta x + \ltlo{\delta x} + \sqbkt{f_y(x, y) + f_{yx}(x, y)\delta x + \ltlo{\delta x}}\delta y + \ltlo{\delta y}
\]
Note that taylors theorem allowed us to remove the dependance of $\delta x$ and $\delta y$ from the function. We can now use the differential of $f$.

\begin{defi}[Differential]
    The \emph{differential} of $f$ is defined to be
    \[
        \diffd f = \lim_{\substack{\delta x \rightarrow 0 \\ \delta y \rightarrow 0}} \delta f  
    \]
\end{defi}
Applying this get
\begin{align*}
    \diffd f &= \lim_{\substack{\delta x \rightarrow 0 \\ \delta y \rightarrow 0}} f_x(x, y)\delta x + \ltlo{\delta x} + f_y(x, y)\delta y + f_{yx}(x, y)\delta x\delta y + \ltlo{\delta x}\delta y + \ltlo{\delta y} \\
    &= f_x(x, y) \lim_{\delta x \rightarrow 0} \delta x + f_y(x, y) \lim_{\delta y \rightarrow 0}\delta y \\
    &= f_x(x, y) \diffd x + f_y(x, y) \diffd y
\end{align*}

\begin{thm}[Chain rule for partial differentiation]
    The differential of $f(x, y)$ is related to the differential of its arguments by,
    \[
        \diffd f = \diffp{f}{x} \diffd x + \diffp{f}{y} \diffd y
    \] 
    and more generally, for a function $f$ whose arguments are $x_1, x_2, \cdots, x_n$,
    % \[
    %     \diffd f = \diffp{f}{x_1} \diffd x_1 + \ldots + \diffp{f}{x_n} \diffd x_n
    % \]
    \[
        \diffd f = \diffp{f}{{x_1}} \diffd x_1 + \ldots + \diffp{f}{{x_n}} \diffd x_n
    \]
\end{thm}

Hence for the path $x(t), y(t)$
\[
    \diff{f}{t} = \diffp{f}{x} \diff{x}{t} + \diffp{f}{y} \diff{y}{t}
\]
Furthermore, if the path is parameterised by a coordinate, for instance $y = f(x)$, then we simply have a function in one variable and
\[
    \diff{f}{x} = \diffp{f}{x} + \diffp{f}{y} \diff{y}{x}
\]

\begin{defi}[Integral form]
    If we define $\Delta f$ as the change in height between the two endpoints we have,
    \[
        \Delta f = \int{\diffd f} = \int{\bkt{\diffp{f}{x} \diffd x + \diffp{f}{y} \diffd y}}  
    \]
    In the case that $x(t)$ and $y(t)$,
    \[
        \Delta f = \int{\diffd f} = \int{\bkt{\diffp{f}{x} \diff{x}{t} + \diffp{f}{y} \diff{y}{t}} \diffd t}  
    \]
\end{defi}
\subsection{Applications of multivariate chain rule}

We can use the multivariate chain rule to help us change variables. For instance consider the map from cartesian coordintes to plane-polar coordinates.
\[
    (x, y) \mapsto (r, \theta)
\]

% insert diagram
Then we have the relationships
\[
    x = r\cos{\theta} \qquad y = r\sin{\theta}
\]
and we can reinterprit the function $f(x, y)$ as a function $f(x(r, \theta), y(r, \theta))$ in the plane-polar coordinates. In this case,
\begin{align*}
    \diffp{f}{r} \Big{|}_{\theta} &= \diffp{f}{x}\diffp{x}{r}\Big{|}_{\theta} + \diffp{f}{y}\diffp{y}{r}\Big{|}_{\theta} \\
    &= \diffp{f}{x}\cos{\theta} + \diffp{f}{y}\sin{\theta}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%
% Lecture 5: __/10/2023
%%%%%%%%%%%%%%%%%%%%%%


\end{document}