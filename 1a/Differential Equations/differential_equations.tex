\documentclass{article}

\def\npart {IA}
\def\nterm {Michaelmas}
\def\nyear {2023}
\def\nlecturer {Prof A.\ D.\ Challinor}
\def\ncourse {Differential Equations}

\input{../../header.tex}

\begin{document}
\maketitle{
    \small
    \noindent\textbf{Basic calculus}\\
    Informal treatment of differentiation as a limit, the chain rule, Leibnitz's rule, Taylor series, informal treatment of $O$ and $o$ notation and l'H\^opital's rule; integration as an area, fundamental theorem of calculus, integration by substitution and parts.\hspace*{\fill}[3]
  
    \vspace{5pt}
    \noindent Informal treatment of partial derivatives, geometrical interpretation, statement (only) of symmetry of mixed partial derivatives, chain rule, implicit differentiation. Informal treatment of differentials, including exact differentials. Differentiation of an integral with respect to a parameter.\hspace*{\fill}[2]
  
    \vspace{10pt}
    \noindent\textbf{First-order linear differential equations}\\
    Equations with constant coefficients: exponential growth, comparison with discrete equations, series solution; modelling examples including radioactive decay.
  
    \vspace{5pt}
    \noindent Equations with non-constant coefficients: solution by integrating factor.\hspace*{\fill}[2]
  
    \vspace{10pt}
    \noindent\textbf{Nonlinear first-order equations}\\
    Separable equations. Exact equations. Sketching solution trajectories. Equilibrium solutions, stability by perturbation; examples, including logistic equation and chemical kinetics. Discrete equations: equilibrium solutions, stability; examples including the logistic map.\hspace*{\fill}[4]
  
    \vspace{10pt}
    \noindent\textbf{Higher-order linear differential equations}\\
    Complementary function and particular integral, linear independence, Wronskian (for second-order equations), Abel's theorem. Equations with constant coefficients and examples including radioactive sequences, comparison in simple cases with difference equations, reduction of order, resonance, transients, damping. Homogeneous equations. Response to step and impulse function inputs; introduction to the notions of the Heaviside step-function and the Dirac delta-function. Series solutions including statement only of the need for the logarithmic solution.\hspace*{\fill}[8]
  
    \vspace{10pt}
    \noindent\textbf{Multivariate functions: applications}\\
    Directional derivatives and the gradient vector. Statement of Taylor series for functions on $\R^n$. Local extrema of real functions, classification using the Hessian matrix. Coupled first order systems: equivalence to single higher order equations; solution by matrix methods. Non-degenerate phase portraits local to equilibrium points; stability.
    
    \vspace{5pt}
    \noindent Simple examples of first- and second-order partial differential equations, solution of the wave equation in the form $f(x + ct) + g(x - ct)$.\hspace*{\fill}[5]}

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%
% Lecture 1: 6/10/2023
%%%%%%%%%%%%%%%%%%%%%%
\section{Basic Calculus}
\subsection{Differentiation}
\begin{defi}[Derivative of a function]
    Define the derivative of $f(x)$ w.r.t its argument $x$ as the function
    \[
        \diff{f}{x} \equiv \lim_{h \rightarrow 0}{\frac{f(x +h) - f(x)}{h}}
    \]

    % Insert the diagram of the slopy thing.

    Note that for a derivative to exist at point $x$, both the left and right hand limits must exist and be equal. 
    \[
        \lim_{h \rightarrow 0^-}{\frac{f(x +h) - f(x)}{h}} = \lim_{h \rightarrow 0^+}{\frac{f(x +h) - f(x)}{h}}
    \]
\end{defi}

\begin{eg}[$f(x) = \abs{x}$]
    \begin{align*}
        \text{LHL} &= \lim_{h \rightarrow 0^-}{\frac{\abs{h}}{h}} = -1 \\
        \text{RHL} &= \lim_{h \rightarrow 0^-}{\frac{\abs{h}}{h}} = 1
    \end{align*}
    We see that $\abs{x}$ is not differentiable at $x=0$
\end{eg}

Some notational differences. Most often Newton's notation is used to denote differentiation with respect to time. 
\begin{center}
    \begin{tabular}{ c|c|c }    
        Leibnitz & Lagrange & Newton \\
        \midrule
        $\diff{f}{x}$ & $f^{\prime}(x)$ & $\dot{f}(x)$
    \end{tabular}
\end{center}

For sufficiently smooth functions, we can define higher derivatives. Here are some second derivatives
\[
    \diff{}{x}\left(\diff{f}{x}\right) = \diff[2]{f}{x} \quad \text{or} \quad f^{\prime\prime}(x) \quad \text{or} \quad \ddot{f}(x)
\]
For $n^{th}$ derivatives we tend to use this notation
\[
    f^{(n)}(x)
\]

\subsection{Big $O$ and little $\underline{o}$ notation}.
These are known as order parameters and are very useful when comparing the behaviour of 2 functions sufficiently close to some point $x_o$ or as $x$ tends to infinity. They are particular useful in deal with approximations where the remainder depends upon a parameter.

\begin{defi}[$O$ and $\underline{o}$]\leavevmode
    \begin{enumerate}
        \item $f(x)$ is $O(g(x))$ as $x \rightarrow x_0$ if $\exists \delta > 0$ and $M > 0$ such that $\forall x$ with $0 < \abs{x - x_0} < \delta$,
        \[
            \abs{f(x)} \leq M\abs{g(x)}  
        \]
        It follows from this that $\frac{f(x)}{g(x)}$ remains bounded as $x \rightarrow x_0$
        \item $f(x)$ is $\underline{o}(g(x))$ as $x \rightarrow x_0$ if $\forall \epsilon > 0, \exists \delta > 0$ such that $\forall x$ with $0 < \abs{x - x_0} < \delta$,
        \[
            \abs{f(x)} \leq \epsilon\abs{g(x)}  
        \]
        This means that $f(x)$ needs to be much smaller than $g(x)$. It follows similarly that if $g(x) \neq 0 $
        \[
            \lim_{x \rightarrow x_0}{\frac{f(x)}{g(x)}} = 0  
        \]
    \end{enumerate}
    We often write that $f(x) = O(g(x))$ or $f(x) = \underline{o}(g(x))$, but this is an abuse of notation. A more accurate notation would be to write
    \[
        f(x) \in O(g(x)) \quad f(x) \in \underline{o}(g(x))
    \]
    Since these actually represent classes of functions. We can also notice that $\underline{o}$ is a much stronger statement than $O$ since, for $\underline{o}$ we need to be able to produce a value of $x$ that results in an arbitrarily small multiple of $g(x)$

    We can also extend this to behaviour as $x \rightarrow \infty$. We say that $f(x) = O(g(x))$ if $\exists X > 0$ and $M > 0$ such that $\forall x > X$,
    \[
        \abs{f(x)} \leq M\abs{g(x)}  
    \]

    Note that
    \[
        f(x) = \underline{0}(g(x)) \Rightarrow f(x) = O(g(x))  
    \]
\end{defi}

\begin{eg}\leavevmode
    \begin{enumerate}
        \item Given $f(x) = 2x, \quad f(x) = O(x)$ as $x \rightarrow 0$. However, $f(x) \neq \underline{o}(g(x))$ as $x \rightarrow 0$ since,
        \[
            \lim_{x \rightarrow 0}{\abs{\frac{f(x)}{x}}} = 2 \neq 0  
        \]
        \item Given $f(x) = x^2, \quad f(x) = \underline{o}(x)$ as $x \rightarrow 0$ since, 
        \[
            \lim_{x \rightarrow 0}{\abs{\frac{x^2}{x}}} = 0
        \]
        \item Given $f(x) = x^2 + x, \quad f(x) = O(x^2)$ as $x \rightarrow \infty$ since for any $x > 1$,
        \[
            \abs{x^2 + x} \leq 2\abs{x^2}  
        \]
        In general, any polynomial where $a_n \neq 0$
        \[
            a_nx^n + a_{n-1}x^{n-1} + \cdots + a_1x + a_0 = O(x^n) \quad \text{as} \quad x \rightarrow 0
        \]
        \item Given $f(x) = \sin{2x}, \quad f(x) = O(x)$ as $x \rightarrow 0$ since, $\sin{2x} \approx 2x$ for $x << 1$
    \end{enumerate}
\end{eg}

\begin{remark}
    When using order paramters, constants do not matter. If 
    \begin{align*}
        f(x) &= O(g(x)) \Rightarrow af(x) = O(g(x)) \\
        f(x) &= O(ag(x)) \quad \text{for} \ a \neq 0
    \end{align*}
\end{remark}

Order paramters are useful to classify remainder terms before taking a limit. Consider the following expression
\[
    f(x_0 + h) - f(x_0) = h f^{\prime}(x_0) + \varepsilon(h) 
\]
By comparing this with the definition of the derivative, we can take a limit
\begin{align*}
    \lim_{h \rightarrow 0}{\frac{f(x_0 + h) - f(x_0)}{h}} &= f^{\prime}(x_0) + \lim_{h \rightarrow 0}{\varepsilon(h)} \\
    &\therefore  \lim_{h \rightarrow 0}{\varepsilon(h)} = 0 \\
    &\Rightarrow \varepsilon(h) = \underline{o}(h)
\end{align*}
\[
    \therefore f(x_0 + h) = f(x_0) + h f^{\prime}(x_0) + \underline{o}(h)
\]

%%%%%%%%%%%%%%%%%%%%%%
% Lecture 2: 8/10/2023
%%%%%%%%%%%%%%%%%%%%%%

\subsection{Rules for differentiation}
\begin{thm}[Chain Rule]
    Given $f(x) = F(g(x))$,
    \[
        \diff{f}{x} = F^{\prime}(g(x)) \cdot \diff{g}{x}   = \diff{F}{g}\diff{g}{x}
    \]
    Note that $F^{\prime}(g(x))$ refers to the derivative of the function $F$ with respect to its argument $g$ and then evaluated at $g(x)$
\end{thm}

\begin{proof}[See notes]
    
\end{proof}

\begin{eg}
    \[
        \diff{}{x} \sin{(x^2 -x +2)} = \cos{(x^2 -x +2)}[2x - 1]
    \]
\end{eg}

\begin{thm}[Product Rule]
    Given $f(x) = u(x)v(x)$,
    \[
        \diff{f}{x} = v \diff{u}{x} + u \frac{dv}{dx}
    \]
    The Quotient rule is a special case of the product rule, when $v \rightarrow \frac{1}{v}$
    \begin{align*}
        \diff{}{x}\left(\frac{u}{v}\right) &= \frac{1}{v}\diff{u}{x} + u \diff{}{x}\left(\frac{1}{v}\right) \\
        &= \frac{1}{v}\diff{u}{x} + \frac{u}{v^2} \frac{dv}{dx} \\
        &= \frac{vu^{\prime} - v^{\prime}u}{v^2}
    \end{align*}
\end{thm}

\begin{thm}[Leibnitz'z Rule]
    Consider $f(x) = u(x)v(x)$ then,
    \begin{align*}
        f^{\prime} &= u^{\prime}v + uv^{\prime} \\
        f^{\prime\prime} &= u^{\prime\prime}v + 2u^{\prime}v^{\prime} + uv^{\prime\prime} \\
        f^{\prime\prime\prime} &= u^{\prime\prime\prime}v + 3u^{\prime\prime}v^{\prime} + 3u^{\prime}v^{\prime\prime} + uv^{\prime\prime\prime} \\
        \vdots
    \end{align*}

    We have that
    \[
        f^{(n)}(x) = \sum_{r=0}^{n}{\binom{n}{k}u^{(n-r)}(x)v^{r}(x)}
    \]
    Note that
    \[
        u^{(0)}(x) \equiv u(x)  
    \]
\end{thm}

\begin{proof}[Exercise by induction]
    
\end{proof}

\subsection{Taylor Series}
A taylor series is an infinite series that tries to approximate a function in the vincity of some point. It is constructed to match the first $n$ differentials of a function.

\begin{defi}[Taylor Series]
    For $f(x)$ infinitely differentiable at $x_0$, the taylor series $T_f(x)$ is defined to be
    \[
        T_f(x) = f(x_0) + (x - x_0)f^{\prime}(x) + \frac{(x - x_0)^2}{2!}f^{\prime\prime}(x) + \cdots + \frac{(x - x_0)^n}{n!}f^{(n)}(x) + \cdots
    \]
\end{defi}

Taylor series are often used for approximations. To consider the errors these approximations have, we look at Taylor's Theorem.

\begin{thm}[Taylor's Theorem]
    For $n$-times differentiable $f(x)$ at $x_0$,
    \[
        f(x_0 + h) = f(x_0) + hf^{\prime}(x_0) + \frac{h^2}{2!}f^{\prime\prime}(x_0) + \cdots + \frac{h^n}{n!}f^{(n)}(x) + E_n
    \]
    Where $E_n$ accounts for the error in the approximation. Taylor's theorem states that
    \[
        E_n = \underline{o}(h^n) \quad \text{as} \quad x \rightarrow 0  
    \]
\end{thm}

A stronger form of this exists, if $f^{(n+1)}(x)$ exists $\forall x \in [x_0, x_0 + h]$ as in continuous in this range,
\begin{align*}
    E_n &= O(h^{n+1}) \quad \text{as} \quad x \rightarrow 0 \\
    &= \frac{f^{n+1}(x_n)}{(n+1)!}h^{n+1} \quad \text{for} \quad x \in [x_0, x_0 +h]
\end{align*}
This is due to Lagrange. Note that $E_n = O(h^{n+1})$ is stronger than $\underline{o}(h^n)$. For instance, consider $h^{n + 0.5} = \underline{o}(h^n)$ but $\neq O(h^{n+1})$ as $h \rightarrow 0$

\begin{defi}[Taylor Polynomials]
    With $x = x_0 + h$ and Taylor's theorem gives
    \[
        f(x) = f(x_0) + (x - x_0)f^{\prime}(x) + \frac{(x - x_0)^2}{2!}f^{\prime\prime}(x) + \cdots + \frac{(x - x_0)^n}{n!}f^{(n)}(x) + E_n
    \]
    This is called the ``$n^{th}$'' Taylor polynomial about $x_0$. It is constructed to match the first $n$ derivatives of $f(x)$ as $x_0$ and provides a local approximation to $f(x)$ in the vicinity of $x_0$ with error $E_n = O(h^{n+1})$. If we have that
    \[
        \lim_{n \rightarrow \infty}{E_n} = 0  
    \]
    then the Taylor Polynomial converges to $f(x)$
\end{defi}

\subsection{L'H\^opital's Rule}
This theorem allows us to deal with limits of \emph{indeterminant forms}. For example $\lim_{x \rightarrow x_0}{\frac{f(x)}{g(x)}}$ where $\lim_{x \rightarrow x_0}{f(x)} = \lim_{x \rightarrow x_0}{g(x)} = 0$.

\begin{thm}[L'H\^opital's Rule]
    Let $f(x)$ and $g(x)$ be differentiable at $x_0$ and have continuous first derivatives there, and
    \begin{align*}
        \lim_{x \rightarrow x_0}{f(x)} &= f(x_0) = 0 \\
        \lim_{x \rightarrow x_0}{g(x)} &= g(x_0) = 0
    \end{align*}
    Then if $g^{\prime}(x_0) \neq 0$,
    \[
        \lim_{x \rightarrow x_0}{\frac{f(x)}{g(x)}} = \lim_{x \rightarrow x_0}{\frac{f^{\prime}(x)}{g^{\prime}(x)}}
    \]
    Provided that the second limit exists. Note that the non-existence of the second limit does not give any information about the original limit.
\end{thm}

\begin{proof}
    Using the result at the end of the section discussing order parameters,
    \begin{align*}
        f(x) &= f(x_0) + (x - x_0)f^{\prime}(x_0) + \underline{o}(x - x_0) \quad \text{as} \quad x \rightarrow x_0 \\
        g(x) &= g(x_0) + (x - x_0)g^{\prime}(x_0) + \underline{o}(x - x_0) \quad \text{as} \quad x \rightarrow x_0
    \end{align*}
    \[
        \lim_{x \rightarrow x_0}{\frac{f(x)}{g(x)}} = \lim_{x \rightarrow x_0}{\frac{f^{\prime}(x_0) + \frac{\underline{o}(x - x_0)}{x - x_0}}{g^{\prime}(x_0) + \frac{\underline{o}(x - x_0)}{x - x_0}}}
    \]
    Since we have that $g^{\prime}(x_0) \neq 0$, 
    \begin{align*}
        \lim_{x \rightarrow x_0}{\frac{f(x)}{g(x)}} &= \frac{f^{\prime}(x_0)}{g^{\prime}(x_0)} \\
        &= \frac{\lim_{x \rightarrow x_0}{f^{\prime}(x)}}{\lim_{x \rightarrow x_0}{g^{\prime}(x)}} \tag{assuming continuous first derivatives}\\
        &= \lim_{x \rightarrow x_0}{\frac{f^{\prime}(x)}{g^{\prime}(x)}} \tag{already checked $g(x_0) \neq 0$}
    \end{align*}
\end{proof}

We can generalise, by repeating L'\^opital's Rule, given that the conditions hold. For instance, we can have if $f^{\prime}(x_0) = g^{\prime}(x_0) = 0$
\[
    \lim_{x \rightarrow x_0}{\frac{f(x)}{g(x)}} = \frac{f^{\prime\prime}(x_0)}{g^{\prime\prime}(x_0)}
\]

\begin{eg}
    \begin{align*}
        f(x) &= 3\sin{x} - \sin{3x}
        g(x) = 2x - \sin{2x}
    \end{align*}
\end{eg}

%%%%%%%%%%%%%%%%%%%%%%
% Lecture 3: 11/10/2023
%%%%%%%%%%%%%%%%%%%%%%

\section{Integration}
\subsection{Reimann sums}
We will formalise the idea of taking the area under a function. 

\begin{defi}[Integral]
    The integral of a suitably well-defined function $f(x)$ is the limit of a sum
    \begin{align*}
        \int_{a}^{b}{f(x) \ \diffd x} &\equiv \lim_{N \rightarrow \infty}{\sum_{n=0}^{N-1}{f(x_n)\Delta x}} \\
        &= \lim_{\Delta x \rightarrow 0}{\sum_{n=0}^{N-1}{f(x_n)\Delta x}}
    \end{align*}
    Where $\Delta x = \frac{b - a}{N}$ and $x_n = a + n\Delta x$
    
    % Insert diagram of reimann sums
    Note that the $\lim_{N \rightarrow \infty}$ is independant on how the rectangles are drawn. For instance, we can have $\Delta x$ be non-uniform. (see Analysis I)
\end{defi}

As we have drawn, it appears that each partial sum will underestimate the limit of the area.
\begin{prop}
    We will show that in the limit as the $\Delta x \rightarrow 0$, that the difference between the area and the limit approaches $0$.
\end{prop}
\begin{proof}
    Consider one rectangle for finite $N$. 
    % insert diagram
    Using the mean value theorem, there exists a $c$ in the range $x_n \leq c \leq x_{n+1}$ such that the area $A_n$
    \[
        A_n = (x_{n+1} - x_n)f(c)  
    \]
    If $f(x)$ is differentiable we have from Taylor's theorem that
    \[
        f(c) = f(x_n) + O(c - x_n), \quad \text{as} \ c-x_n \rightarrow 0  
    \]
    Since $\Delta x$ is larger than $c - x_n$,
    \[
        f(c) = f(x_n) + O(\Delta x)
    \]
    Thus it follows that
    \[
        A_n =  \Delta x f(x_n) + \Delta x O(\Delta x), \quad \text{as} \ \Delta x \rightarrow 0 
    \]
    Note that
    \[
        \Delta x O(\Delta x) = O(\Delta x^2)
    \]
    The total area between $x=a$ and $x=b$ is therefore,
    \[
        A = \lim_{N \rightarrow \infty}{\sum_{n=0}^{N-1}{A_n}} = \lim_{N \rightarrow \infty}{\sum_{n=0}^{N-1}{\Delta x f(\Delta x)}} + \lim_{N \rightarrow \infty}{\sum_{n=0}^{N-1}{O(\Delta x^2)}}
    \]
    Note that
    \[
        \lim_{N \rightarrow \infty}{\sum_{n=0}^{N-1}{\Delta x f(\Delta x)}}
    \]
    Is the definition of the reimann sum and therefore the error is given by
    \begin{align*}
        \lim_{N \rightarrow \infty}{\sum_{n=0}^{N-1}{O(\Delta x^2)}} &= \lim_{N \rightarrow \infty}{NO \bkt{\bkt{\frac{b-a}{N}}^2}} \\
        &= \lim_{N \rightarrow \infty}{O\bkt{\frac{(b-a)^2}{N}}} \\
        &= 0
    \end{align*}
\end{proof}

\subsection{Fundamental Theorem of Calculus}
We will formalise the idea of taking the inverse of differentiation.

\begin{thm}[FTC]
    Let $F(x)$ be defined as
    \[
        F(x) = \int_{a}^{x}{f(t) \ \diffd t}  
    \]
    Then,
    \[
        \diff{F}{x} = f(x)
    \]
\end{thm}
\begin{proof}
    \begin{align*}
        \diff{F}{x} &= \lim_{h \rightarrow 0}\frac{F(x + h) - F(x)}{h} \\
        &= \lim_{h \rightarrow 0}\frac{1}{n}\sqbkt{\int_{x}^{x+h}{f(t) \ \diffd t}} \\
        &= \lim_{h \rightarrow 0}\frac{1}{n}\sqbkt{f(x)h + O(h^2)} \tag{from reimann sum definition} \\
        &= f(x) + \lim_{h \rightarrow 0}O(h^2) \\
        &= f(x)
    \end{align*}
\end{proof}

\begin{remark}
    $F(x)$ is a solution of the differential equation $\diff{F}{x} = f(x)$ with the added property that $F(a) = 0$
\end{remark}

\begin{cor}
\[
    \diff{}{x}\int_{x}^{b}{f(x) \ \diffd t} = -f(x)
\]
and by the chain rule, 
\[
    \diff{}{x}\int_{a}^{g(x)}{f(x) \ \diffd t} = f(g(x))g'(x)
\]
\end{cor}

\begin{notation}
    We typically write indefinite integrals as $\int{f(x) \ \diffd x}$. We may also write
    \[
        \int^{x}{f(t) \ \diffd t}    
    \]
    wich is arguably better as it makes clear that the result is a function of $x$. Further note that the undefined lower limit leaves space for the constant of integration $c$.
\end{notation}

\subsection{Methods of Integration}

\begin{defi}[Substitution]
    Substitution is useful if the integrand contains composed functions, or if we can spot the derivative of a function and itself.
\end{defi}
\begin{eg}
    \[
        I = \int{\frac{1 - 2x}{\sqrt{x - x^2}} \ \diffd x}
    \]
    Let $u = x - x^2$, $\diffd u = (1 - 2x) \diffd x$
    \begin{align*}
        I &= \int{\frac{1}{\sqrt{u}} \ \diffd u} \\
        &= 2\sqrt{u} \\
        &= 2\sqrt{x - x^2} + x
    \end{align*}
\end{eg}
Here are some useful substitution to consider, motivated by the trigonometric and hyperbolic identities
\begin{center}
    \begin{tabular}{c | c}
        Term in integrand & Substitution \\
        \midrule
        $1 - x^2$ & $x = \sin{\theta} or \tanh{\theta}$\\
        $1 + x^2$ & $x = \tan{\theta} or \sinh{\theta}$\\
        $x^2 - 1$ & $x = \sec{\theta} or \cosh{\theta}$\\

    \end{tabular}
\end{center}

\begin{eg}
    \[
        I = \int{\sqrt{x - x^2} \ \diffd x} = \int{\sqrt{1 - (x - 1)^2} \ \diffd x}    
    \]
    Let $x - 1 = \sin{\theta}$, $\diffd x = \cos{\theta} \diffd \theta$
    \begin{align*}
        I &= \int{\cos^2{\theta} \ \diffd \theta} \\
        &= \int{\frac{\cos{2\theta} + 1}{2}} \ diffd \theta \\
        &= \frac{1}{4}\sin{2\theta} + \frac{1}{2}\theta + c \\
        &= \frac{1}{2}\sin^{-1}{(x-1)} + \frac{1}{2}(x-1)\sqrt{2x - x^2} + c
    \end{align*}
\end{eg}

\begin{defi}[By parts]
    Integration by parts follows from the product rule. Recall
    \[
        (uv)' = uv' + u'v
    \]
    We can then rearrange
    \[
        uv' = (uv)' - u'v \Rightarrow \int{uv' \diffd x} = uv - \int{u'v \diffd x}
    \]
\end{defi}

\begin{eg}\leavevmode
    \begin{enumerate}
        \item \begin{align*}
            I = \int_{0}^{\infty}{xe^{-x} \diffd x} &= \left. -xe^{-x} \right\rvert_{0}^{\infty} + \int_{0}^{\infty}{e^-x} \\
            &= 0 + 1 = 1
        \end{align*}
        \item \[
            I = \int{\ln{x} \diffd x} = x\ln{x} - x + c    
        \]
    \end{enumerate}
\end{eg}
\end{document}