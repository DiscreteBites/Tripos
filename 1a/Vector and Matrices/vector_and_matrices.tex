\documentclass{article}

\def\npart {IA}
\def\nterm {Michaelmas}
\def\nyear {2023}
\def\nlecturer {Prof N.\ Peake}
\def\ncourse {Vector and Matrices}

\input{../../header.tex}
\numberwithin{equation}{section}

\begin{document}
\maketitle{
    \small 
    \noindent\textbf{Complex numbers}\\
    Review of complex numbers, including complex conjugate, inverse, modulus, argument and Argand diagram. Informal treatment of complex logarithm, $n$-th roots and complex powers. de Moivre's theorem.\hspace*{\fill}[2]

    \vspace{10pt}
    \noindent\textbf{Vectors}\\
    Review of elementary algebra of vectors in $\R^3$, including scalar product. Brief discussion of vectors in $\R^n$ and $\C^n$; scalar product and the Cauchy-Schwarz inequality. Concepts of linear span, linear independence, subspaces, basis and dimension.

    \vspace{5pt}
    \noindent Suffix notation: including summation convention, $\delta_{ij}$ and $\varepsilon_{ijk}$. Vector product and triple product: definition and geometrical interpretation. Solution of linear vector equations. Applications of vectors to geometry, including equations of lines, planes and spheres.\hspace*{\fill}[5]

    \vspace{10pt}
    \noindent\textbf{Matrices}\\
    Elementary algebra of $3\times 3$ matrices, including determinants. Extension to $n\times n$ complex matrices. Trace, determinant, non-singular matrices and inverses. Matrices as linear transformations; examples of geometrical actions including rotations, reflections, dilations, shears; kernel and image.\hspace*{\fill}[4]

    \vspace{5pt}
    \noindent Simultaneous linear equations: matrix formulation; existence and uniqueness of solutions, geometric interpretation; Gaussian elimination.\hspace*{\fill}[3]

    \vspace{5pt}
    \noindent Symmetric, anti-symmetric, orthogonal, hermitian and unitary matrices. Decomposition of a general matrix into isotropic, symmetric trace-free and antisymmetric parts.\hspace*{\fill}[1]

    \vspace{10pt}
    \noindent\textbf{Eigenvalues and Eigenvectors}\\
    Eigenvalues and eigenvectors; geometric significance.\hspace*{\fill}[2]

    \vspace{5pt}
    \noindent Proof that eigenvalues of hermitian matrix are real, and that distinct eigenvalues give an orthogonal basis of eigenvectors. The effect of a general change of basis (similarity transformations). Diagonalization of general matrices: sufficient conditions; examples of matrices that cannot be diagonalized. Canonical forms for $2 \times 2$ matrices.\hspace*{\fill}[5]

    \vspace{5pt}
    \noindent Discussion of quadratic forms, including change of basis. Classification of conics, cartesian and polar forms.\hspace*{\fill}[1]

    \vspace{5pt}
    \noindent Rotation matrices and Lorentz transformations as transformation groups.\hspace*{\fill}[1]}

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%
% Lecture 1: 6/10/2023
%%%%%%%%%%%%%%%%%%%%%%
\section{Complex Numbers}
\subsection{Basic properties}
\begin{defi}[Complex number]
    A \emph{complex number} $z \in \C$ is
    \[
        z = a + ib, \quad a, b \in \R, \quad i = \sqrt{-1}  
    \]
    We define two other functions, $\Re()$ and $\Im()$ such that,
    \[
        \Re(z) = a \quad \Im(z) = b 
    \]
    Clearly, we may note that $\R \subset \C$ in the case where $b = 0$
\end{defi}

We can define algebraic manipulation over the complex numbers,
\begin{align*}
    z_1 \pm z_2 &= a_1 \pm a_2 + i(b_1 \pm b_2) \\
    z_1z_2 &= a_1a_2 - b_1b_2 + i(a_1b_2 + a_2b_1)
\end{align*}
We can define the inverse of a complex number
\begin{equation}
    z^{-1} = \frac{1}{a + ib} = \frac{a - ib}{a^2 + b^2}\label{eq:1}
\end{equation}
this demonstrates that $\C$ is \emph{closed} under addition and multiplication. We also define a complex conjugate of $z = a + ib$ as $\overline{z}$ or $z^*$.
\[
    \overline{z}, z^* = a - ib
\]
The modulus of $z$ as $\abs{z}$
\begin{align*}
    \abs{z} &= \abs{a + ib} = \sqrt{a^2 + b^2} \\
    z\overline{z} &= a^2 + b^2 = \abs{z}^2 \\
    z^{-1} &= \frac{\overline{z}}{\abs{z}^2} \tag{see \eqref{eq:1}}
\end{align*}
We also introduce the Argand Diagram, where the coordinate axes are the Real and Imaginary axes. We can see that $\overline{z}$ is the reflection of $z$ in the $x$-axis.

% draw an argand diagram in tikz
\begin{defi}[Triangle inequality]
    For $z_1, z_2 \in \C$, the triangle inequality states that
    \begin{equation}
        \abs{z_1 + z_2} \leq \abs{z_1} + \abs{z_2}\label{eq:2}
    \end{equation}
\end{defi}

\begin{prop}
    The triangle inequality has another common alternative form,
    \begin{equation}
        \abs{z_1 - z_2} \geq \abs{\abs{z_1} - \abs{z_2}}\label{eq:3}
    \end{equation}
\end{prop}

\begin{proof}
    Write $z_1 = z^{\prime}_1 - z^{\prime}_2$, and $z_2 = z^{\prime}_2$ and substitute them into \eqref{eq:2}. Then,
    \begin{align*}
        &\abs{z^{\prime}_1} \leq \abs{z^{\prime}_1 - z^{\prime}_2} + \abs{z^{\prime}_2} \\
        \Rightarrow& \abs{z^{\prime}_1} - \abs{z^{\prime}_2} \leq\abs{z^{\prime}_1 - z^{\prime}_2}\\
        \Rightarrow& \abs{z^{\prime}_1 - z^{\prime}_2} \geq \abs{z^{\prime}_1} - \abs{z^{\prime}_2} \tag{$\star$}
    \end{align*}
    Notice that our labelling of $z_1$ and $z_2$ was arbitrary, and so we can freely swap the assignment. This gives,
    \[
        \abs{z^{\prime}_1 - z^{\prime}_2} \geq \abs{z^{\prime}_2} - \abs{z^{\prime}_1} \tag{$\star\star$}
    \]
    Since $\abs{z^{\prime}_1 - z^{\prime}_2} \geq (\star) \ \text{and} \ (\star\star)$, hence, \eqref{eq:3}
\end{proof}

\begin{defi}[Polar Representation]
    For a complex number $z = a + ib$ we use the argand diagram to write $z = r(\cos{\theta} + i\sin{\theta})$, where
    \[
        r = \abs{z} \quad \text{and} \quad \theta = \arctan\left(\frac{b}{a}\right) = \arg{(z)}  
    \]
\end{defi}

\begin{remark}
    Note that the mapping $(r, \theta) \rightarrow z$ is unique. However, the reverse $r \rightarrow (r, \theta)$ is \emph{NOT} unique since for each value of $\theta$, $\theta + 2n\pi$ is equally valid.

    In order to make this mapping single valued, we will by convention restrict $\theta$ to the principle value, where
    \[
        -\pi < \theta \leq \pi  
    \]

    This multivalued property becomes quite interesting when we consider complex functions. We can imagine taking the multivalued imaginary $\ln$ function as a multistory carpark, walking round and round up and down. To fix this what we do is to build a brick wall on the upramp (``branch cut'').
\end{remark}

Using complex numbers in this form allows us to view complex arithmetic geometrically. For instance, let
\begin{align*}
    z_k &= x_k + iy_k \quad \text{for} \quad k = 1, 2 \\
    &= r_k(\cos{\theta_k} + i\sin(\theta_k))
\end{align*}
\begin{align*}
    z_1z_2 &= r_1(\cos{\theta_1} + i\sin{\theta_1})r_2(\cos{\theta_2} + i\sin{\theta_2}) \\
    &= r_1r_2[(\cos{\theta_1}\cos{\theta_2} - \sin{\theta_1}\sin{\theta_2}) + i(\cos{\theta_1}\sin{\theta_2} + \sin{\theta_1}\cos{\theta_2})] \\
    &= r_1r_2(\cos{(\theta_1 + \theta_2)} + i\sin{(\theta_1 + \theta_2)}) \numberthis \label{eq:4}
\end{align*}

\subsection{Complex exponential function}
\begin{defi}[Complex exponential function]
    \begin{align*}
        \exp(z) &= 1 + z + \frac{z^2}{2!} + \cdots + \frac{z^n}{n!} + \cdots \\
        &=\sum_{n=0}^{\infty}{\frac{z^n}{n!}} \numberthis \label{eq:5}
    \end{align*}
\end{defi}
This function is ``uniformly convergent'' (something that we will cover next term in analysis). That means that we are able to do lots of algebraic manipulation without too much hesitation. =

\begin{prop}[Multiplication of $\exp()$]
    \[
        \exp(z_1)\exp(z_2) = \exp(z_1 + z_2)  
    \]
\end{prop}
\begin{proof}
    We will consider the double sum
    \begin{align*}
        \sum_{m,n=0}^{\infty}{a_mn} &= a_{00} + a_{01} + a_{02} + a_{03} + \cdots \\
         &+ a_{10} + a_{11} + a_{12} + \cdots \\
         &+ a_{20} + a_{21} + a_{22} + \cdots \\
         &+ a_{30} + a_{31} + a_{32} + \cdots \\
         &\vdots
    \end{align*}
    Then by collecting terms along the diagonal we can regroup the sum into
    \[
        =\sum_{r=0}^{\infty}\sum_{m=0}^{r}{a_{r-m, m}}
    \]
    Hence, 
    \begin{align*}
        \exp(z_1) \cdot \exp(z_2) &= \sum_{r=0}^{\infty}\sum_{m=0}^{r}{\frac{z_1^{r-m}}{(r-m)!}\frac{z_2^{m}}{m!}} \\
        &= \sum_{r=0}^{\infty}{\frac{1}{r!}}\sum_{m=0}^{r}{\frac{r!}{(r-m)!m!}z_1^{r-m}z_2^{m}} \\
        &= \sum_{r=0}^{\infty}{\frac{1}{r!} (z_1 + z_2)^r} \\
        &= \exp(z_1 + z_2)
    \end{align*}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%
% Lecture 2: 8/10/2023
%%%%%%%%%%%%%%%%%%%%%%
Similarly, we can define the complex $\sin(z)$ and $\cos(z)$ functions.
\begin{defi}[Complex sine and cosine]
    \begin{align*}
        \sin{z} &= \sum_{n=0}^{\infty}{(-1)^n\frac{z^{2n+1}}{(2n+1)!}} \\
        \cos{z} &= \sum_{n=0}^{\infty}{(-1)^n\frac{z^{2n}}{(2n)!}} \numberthis \label{eq:6}
    \end{align*}
\end{defi}

From \eqref{eq:5} we can derive that
\begin{align*}
    e^{iz} &= \sum_{n=0}^{\infty}{\frac{i^nz^n}{n!}} \\
    &= \sum_{n=0}^{\infty}{(-1)^n\frac{z^{2n+1}i^{2n+1}}{(2n+1)!}} + \sum_{n=0}^{\infty}{(-1)^n\frac{z^{2n}i^{2n}}{(2n)!}} \\
    &= i\sin{z} + \cos{z} \numberthis \label{eq:7}
\end{align*}

We may consider $\theta \in \R$ which gives,
\begin{equation}
    z = re^{i\theta} = r(\cos{\theta} + i\sin{\theta}) \label{eq:8}
\end{equation}

\subsection{Roots of unity}
We seek to solve equations of the form $z^n = 1$ where $n \in \Z$. Since this is a polynomial of degree $n$ we expect to find $n$ solutions. By applying the exponential form we found earlier,
\begin{align*}
    z^n = 1 &= \exp(i2 \pi k), \quad k \in \Z \\
    z &= \exp(i2 \pi \frac{k}{n})
\end{align*}
Notice that the roots will repeat after $k = n-1$. Hence the solutions are
\[
    z = \exp(i2 \pi \frac{k}{n}), \quad k = 0, 1, 2, \cdots, n-1
\]

\begin{eg}[$ z^7 = 2$]
    \[
        z = 2^{\frac{1}{7}} \exp(i 2\pi \frac{m}{7}), \quad m = 0, 1, 2, \cdots, 6
    \]
\end{eg}

An interesting point to note is that for the polynomial $z^n - 1 = 0$, if we arbitrarily choose a root $w = \exp(\frac{2\pi i}{n})$, then all the remaining roots will be
\[
    1, w, w^2, w^3, \cdots, w^{n-1}
\]
Further, if we note that since the coefficient of $z^{n-1}$ is $0$, that implies that the sum of the roots must be $0$.
\[
   1 + w + w^2 + w^3 + \cdots + w^{n-1} = 0
\]

\subsection{Complex Logarithm and Powers}
\begin{defi}[Complex Log]
    The \emph{complex log}, $\log{z}$, is a solution of the following
    \[
        e^w = z  
    \]
    Hence, $\exp()$ and $\log()$ are inverse functions, i.e;
    \[
        e^{\log{z}} = z
    \]
    If we write $z = re^{i\theta}$ from \eqref{eq:8} then,
    \begin{align*}
        \log{z} = \log{re^{i\theta}} &= \log{r} + \log{e^{i\theta}} \\
        &= log{\abs{z}} + i\arg{(z)} \numberthis \label{eq:9}
    \end{align*}
    It is important to note that this is not a single valued function. In order to define a single valued $\log$ we will insist that
    \[
        -\pi < \arg{(z)} \leq \pi
    \]
    Hence,
    \begin{equation}
        \log{z} = log{\abs{z}} + i\theta, \quad -\pi < \theta \leq \pi \label{eq:10}
    \end{equation}
\end{defi}

\begin{eg}
    \begin{align*}
        \log{(2i)} &= \log{2e^{i\frac{\pi}{2}}} \\
        &=\log{2} + i\frac{\pi}{2}
    \end{align*}
\end{eg}

\begin{defi}[Complex Power]
    Since we have defined the complex logarithm we can now define the complex power, $z^\alpha$ for $z, \alpha \in \C$ as
    \begin{equation}
        z^\alpha = e^{\alpha \log{z}} \label{eq:11}
    \end{equation}
    Again, this function is multivalued. Specifically using \eqref{eq:10},
    \[
        z^\alpha = e^{\alpha \log{z}}e^{i \alpha \theta}e^{i \alpha 2\pi n}, \quad n \in \Z
    \]
    However, unlike the roots of unity, there is no reason why this series will repeat and terminate. If $\alpha$ is real and rational then will there be finite solutions. Once again, we will make this function single valued by insisting on using the principle value for $\theta$
    \[
        -\pi < \arg{(z)} \leq \pi
    \]
\end{defi}

\begin{ex}\leavevmode
    \begin{enumerate}
        \item \begin{align*}
            (1 + i)^\frac{1}{2} &= e^{\frac{1}{2}\sqrt{2}}e^\frac{i\pi}{8}e^{in\pi}, \quad n = 0, 1 \\
            &= 2^\frac{1}{4}e^\frac{i\pi}{8}, 2^\frac{1}{4}e^\frac{i9\pi}{8} \\
            &= 2^\frac{1}{4}e^\frac{i\pi}{8} \tag{by insiting on the principle value}
        \end{align*}
        \item \begin{align*}
            i^i &= \exp(i \log{(e^\frac{i\pi}{2})}) \tag{from \eqref{eq:11}} \\
            &= \exp(i \frac{i\pi}{2}) \\
            &= \exp(-\frac{\pi}{2})
        \end{align*}
    \end{enumerate}
\end{ex}

\subsection{De Moivre's Theorem}
\begin{thm}[De Moivre's Theorem]
    \begin{equation}
        \cos{n\theta} + i\sin{n\theta} = (\cos{\theta} + i\sin{\theta})^n, \quad n \in \Z \label{eq:12}
    \end{equation}
\end{thm}

\begin{proof}
    By induction, we will first consider the case where $n > 0$. Clearly, $n=1$ is true. Assume $n=k$ is true for $k > 1$.
    \begin{align*}
        (\cos{\theta} + i\sin{\theta})^{k+1} &= (\cos{k\theta} + i\sin{k\theta})(\cos{\theta} + i\sin{\theta}) \\
        &= \cos{k\theta}\cos{\theta} - \sin{k\theta}\sin{\theta} + i(\cos{k\theta}\sin{\theta} + \sin{k\theta}\cos{\theta}) \\
        &= \cos{(k+1)\theta} + i\sin{(k+1)\theta}
    \end{align*}
    Now when $n < 0$, we write $m = -n$ such that $m > 0$
    \begin{align*}
        (\cos{\theta} + i\sin{\theta})^{-m} &= (\cos{m\theta} + i\sin{m\theta})^{-1} \\
        &= \cos{m\theta} - i\sin{m\theta} \tag{by ($\star$)}\\
        &= \cos{(-m\theta)} - i\sin{(-m\theta)}
    \end{align*}
    Note $(\star)$:
    \[
        z^{-1} = \frac{z^*}{\abs{z}^2}    
    \]
\end{proof}

Using De Moivre's theorem we can work out multiple angle formulae by comparing real and imaginary parts:
\begin{eg}
    \begin{align*}
        \cos{5 \theta} + i\sin{5 \theta} &= (\cos{\theta} + i\sin{\theta})^5 \\
        &\vdots \\
        \cos{5 \theta} &= 5\cos{\theta} - 20 \cos^3{\theta} + 16\cos^5{\theta}\\
        \sin{5 \theta} &= 5\cos{\theta} - 20 \cos^3{\theta} + 16\cos^5{\theta}\\
    \end{align*}
\end{eg}
Note that for non-integer powers of $n$ we will have to insist on the principle value \eqref{eq:12}

\subsection{Lines and Circles in $\C$}
\begin{defi}[Straight Line]
    A line $l$ through $z_0 \in \C$ parallel to $w \in \C$ can be generally written
    \[
        z = z_0 + \lambda w, \quad \lambda \in \R    
    \]
    However, it would be better to get rid of the parameterisation. We can use the fact that $\lambda$ must be real and hence its conjugate is equal to itself.
    \begin{align*}
        \lambda &= \frac{z - z_0}{w} \in \R\\
        &\Rightarrow \frac{z - z_0}{w} = \frac{\overline{z} - \overline{z_0}}{\overline{w}} \\
        &\Leftrightarrow z \overline{w} - \overline{z}w = z_0 \overline{w} - \overline{z_0} w
    \end{align*}
\end{defi}

\begin{defi}[Circle]
    A circle can be defined from its center $c \in \C$ and radius $\rho \in \R$. By definition, if $z$ lies on the circle,
    \begin{align*}
        &\abs{z - c} = \rho \\
        \Rightarrow& (z - c)(\overline{z} - \overline{c}) = \rho^2 \\
        \Leftrightarrow& z\overline{z} - \overline{c}z - c\overline{z} = \rho^2 - c\overline{c}
    \end{align*}
\end{defi}

%%%%%%%%%%%%%%%%%%%%%%
% Lecture 3: 11/10/2023
%%%%%%%%%%%%%%%%%%%%%%

\section{Vectors}
\subsection{Definition \& basic properties}
\begin{defi}[Vector]
    A vector $\vec{x}$ has a positive length and a direction. If,
    \[
        \abs{\vec{x}} = 0 \Rightarrow \vec{x} = \vec{0}  
    \]
    If $\abs{\vec{x}} = 1$ then we write $\uvec{x}$ as \emph{unit vector}. Note the distinction between scalar $0$ and vector $\vec{0}$.
\end{defi}

\begin{defi}[Vector Field]
    A vector field $F = F(\vec{x})$ is a vector function of position and maybe time.
\end{defi}

\begin{defi}[Vector addition]
    % insert paralellogram diagram.
    We can define the addition of vectors using the paralellogram rule,
    \begin{align*}
        \vec{a} + \vec{b} &= \vec{c} \\
        \pvec{OA} + \pvec{OB} &= \pvec{OC}
    \end{align*}
    More abstractly, addition will have the following properties
    \begin{enumerate}[eqn]
        \item $\vec{a} + \vec{b} = \vec{b} + \vec{a}$ \hspace*{\fill}(commutativity) \label{eq:2-1}
        \item $\vec{a} + (\vec{b} + \vec{c}) = (\vec{a} + \vec{b}) + \vec{c}$ \hspace*{\fill}(associativity) \label{eq:2-2}
        \item $\exists \vec{0}, \quad \vec{a} + \vec{0} = \vec{a}$ \hspace*{\fill}(unique zero) \label{eq:2-3}
        \item $\vec{a} + -\vec{a} = 0$ \hspace*{\fill}(unique additive inverse) \label{eq:2-4}
    \end{enumerate}
\end{defi}
\begin{defi}[Multiplication by a scalar]
    For $\lambda \in \R$ (we will do complex scalars later in the course), $\lambda\vec{a}$ has magnitutde $\abs{\lambda}\abs{\vec{a}}$ and is parallel/anti-parallel to $\vec{a}$ if $\lambda<0$ \textbackslash $\lambda>0$. 
    Likewise, we have the following properties
    \begin{enumerate}[eqn]
        \item $(\lambda + \mu)\vec{a} = \lambda\vec{a} + \mu\vec{a}$ \label{eq:2-5}
        \item $\lambda(\vec{a} + \vec{b}) = \lambda\vec{a} + \lambda\vec{b}$ \label{eq:2-6}
        \item $\lambda(\mu\vec{a}) = (\lambda\mu)\vec{a}$ \label{eq:2-7}
        \item $1\vec{a} = \vec{a}$ \label{eq:2-8}
    \end{enumerate}
\end{defi}

Implicit in the two definitions is that the set of vectors is closed under addition and scalar multiplication.

\begin{defi}[Vector Space]
    $V$ is a \emph{vector space} if it is a set of vectors which satisfy properties \eqref{eq:2-1} -- \eqref{eq:2-8} inclusive and is closed under addition and multiplication. Note that for now, we will be considering the vector space $V$ over the real numbers $\R$, but we will consider complex fields later. 
\end{defi}

\begin{eg}[$\R^n$ is a vector space]
\end{eg}
\begin{proof}
    \[
        \R^n = \{\vec{x} = (x_1, x_2, \ldots, x_n) \mid x_j \in \R, \ 1 \leq j \leq n\}
    \]
    Consider $\vec{x} , \vec{y} \in \R$
    \[
        \vec{x} + \vec{y} = (x_1 + y_1, x_2 + y_2, \ldots, x_n + y_n,)
    \]
    From this we see that property \eqref{eq:2-1} and \eqref{eq:2-2} is satisfied as we can inherit them from the Real numbers.

    \begin{enumerate}
        \item $\vec{0} = (0, 0, \ldots, 0)$ \hspace*{\fill}\eqref{eq:2-3}
        \item $-\vec{x} = (-x_1, -x_2, \ldots, -x_n)$ \hspace*{\fill}\eqref{eq:2-4}
        \item $\lambda\vec{x} = (\lambda x_1, \lambda x_2, \ldots, \lambda x_n)$ \hspace*{\fill}\eqref{eq:2-5} - \eqref{eq:2-8}
    \end{enumerate}
\end{proof}

Geometrically, we may interprit $\R^1$ as a line, and $\R^2$ as a plane. However, notice that the converse is not always true. Due to \eqref{eq:2-3} the space has to contain $0$. Therefore, if the line or plane does not go through the origin, it is not a vector space.

\subsection{Scalar Product}
\begin{defi}[Geometric definition in $\R^2$, $\R^3$]
    % insert diagram of dot product
    Given two vectors $\vec{a}, \vec{b}$ we define
    \[
        \vec{a} \cdot \vec{b} = \abs{\vec{a}}\abs{\vec{b}}\cos{\theta}  
    \]
    where $\theta$ is the angle between the two vectors.
\end{defi}
From this, we can see that
\begin{enumerate}
    \item $\vec{a} \cdot \vec{b} = \vec{b} \cdot \vec{a}$ 
    \item $\vec{a} \cdot \vec{a} = 0 \Leftrightarrow \vec{a} = \vec{0}$
    \item $\vec{a} \cdot \vec{b} = 0 \Leftrightarrow$ $\vec{a}$ and $\vec{b}$ are orthogonal
    \[\numberthis\]
\end{enumerate}

\begin{defi}[Projection]
    % insert diagram of projection
    $\vec{b}^\perp$ is the projection of $\vec{b}$ onto $\vec{a}$. Specifically, 
    \begin{equation}
        \vec{b}^\perp = \abs{\vec{b}}\cos{\theta}\frac{\vec{a}}{\abs{\vec{a}}} = \frac{(\vec{b}\cdot\vec{a})\vec{a}}{(\vec{a}\cdot\vec{a})} = (\uvec{a} \cdot \vec{b})\uvec{a} 
    \end{equation}
\end{defi}

We will move to a most abstract defintion of a scalar product
\begin{defi}[Inner product]
    We define $\vec{x} \cdot \vec{y} = \braket{\vec{x}}{\vec{y}} \in \R$ as an \emph{inner product}, if it satisfies
    \begin{enumerate}[eqn]
        \item $\vec{x} \cdot \vec{y} = \vec{y} \cdot \vec{x}$ \hspace*{\fill}(Symmetry) \label{eq:2-11}
        \item $\vec{x}, \vec{y}, \vec{z} \in V, \quad, \lambda, \mu \in \R$ \hspace*{\fill}(Linearity in the second argument) \label{eq:2-12}
        \[
            \vec{x} \cdot (\lambda\vec{y} + \mu\vec{z}) = \lambda\vec{x}\vec{y} + \mu\vec{x}\vec{z}  
        \]
        \item $\vec{x} \cdot \vec{x} \geq 0 $ and $\vec{x} \cdot \vec{x} = 0 \Leftrightarrow \vec{x} = \vec{0}$ \hspace*{\fill}(Positive definite) \label{eq:2-13}
    \end{enumerate}

    Note that we can use \eqref{eq:2-11} together with \eqref{eq:2-12} to derive linearlity in the first argument,
    \begin{equation}
        (\lambda\vec{y} + \mu\vec{z}) \cdot \vec{x} = \lambda\vec{x}\vec{y} + \mu\vec{x}\vec{z} \label{eq:2-15}
    \end{equation}
    We will see later in the course that linearity in the first argument does not hold for vector spaces over the complex field.

    We can also define the \emph{norm} of a vector $\vec{a}$ as 
    \[
        (\vec{a} \cdot \vec{a})^\frac{1}{2} = \abs{\vec{a}} = \norm{\vec{a}}  
    \]
\end{defi}

\begin{remark}
    Using this abstracted definition of an inner product, we can define an inner product between functions. For example define for function $f$ and $g$
    \[
        \braket{f}{g} = \int_{0}^{1}{fg \ \diffd x}
    \]
    This definition satisfies \eqref{eq:2-11} and \eqref{eq:2-12} and we can check \eqref{eq:2-13}
    \[
        \braket{f}{f} = \int_{0}^{1}{f^2 \ \diffd x} \geq 0
    \]
    and further if 
    \[
        \int_{0}^{1}{f^2 \ \diffd x} = 0 \Rightarrow f = 0
    \]
\end{remark}

\subsection{Cauchy-Schwartz Inequality}
\begin{thm}[CS Inequality]
    Given two vectors $\vec{x}$, $\vec{y}$
    \begin{equation}
        \vec{x} \cdot \vec{y} \leq \abs{\vec{x}}\abs{\vec{y}} \quad \vec{x}, \vec{y} \in \R^n
    \end{equation}
    with equality if and only if $\vec{x} = 0$ or $\vec{y} = 0$ or $\vec{x} = \lambda \vec{y}$ for some $\lambda \in \R$.
\end{thm}
\begin{proof}
    Consider $\abs{\vec{x} - \lambda \vec{y}}^2 \geq 0$
    \begin{align*}
        &(\vec{x} - \lambda \vec{y}) \cdot (\vec{x} - \lambda \vec{y}) \geq 0 \\
        \Rightarrow& \lambda^2 \vec{y} \cdot \vec{y} - 2\lambda\vec{x}\vec{y} + \vec{x} \cdot \vec{x} \geq 0
    \end{align*}
    Suppose $\vec{x}, \vec{y} \neq \vec{0}$, we have a quadratic equation in $\lambda$. Therefore since the quadratic is greater than or equal to $0$, the discriminant must be less than or equal to $0$.
    \begin{align*}
        \Delta &= [2(\vec{x} \cdot \vec{y})]^2 - 4 (\vec{y} \cdot \vec{y})(\vec{x} \cdot \vec{x}) \leq 0 \\
        \Rightarrow& [2(\vec{x} \cdot \vec{y})]^2 \leq 4 (\vec{y} \cdot \vec{y})(\vec{x} \cdot \vec{x}) \\
        \Rightarrow& (\vec{x} \cdot \vec{y})^2 \leq (\vec{y} \cdot \vec{y})(\vec{x} \cdot \vec{x}) \\
        \Rightarrow& (\vec{x} \cdot \vec{y})^2 \leq \abs{\vec{x}}^2\abs{\vec{y}}^2
    \end{align*}
    Furthermore, we have equality when
    \[
        \abs{\vec{x} - \lambda \vec{y}}^2 = 0 \Leftrightarrow \vec{x} = \lambda \vec{y}
    \]
    Hence the result
\end{proof}

\begin{eg} $\vec{x} = (\alpha, \beta, \gamma), y = (1, 1, 1)$
    \begin{align*}
        \alpha + \beta + \gamma &\leq \sqrt{\alpha^2 + \beta^2 + \gamma^2} \sqrt{3} \tag{\ref{eq:2-15}} \\
        \alpha^2 + \beta^2 + \gamma^2 &\geq \alpha\beta + \beta\gamma + \gamma\alpha \tag{rearranging}
    \end{align*}
\end{eg}

Note that we must not use the geometric definition of the dot product to prove the CS inequality because $\cos{\theta}$ is not defined for higher dimensions. However, we can use CS to define an angle in $\R^n$
\[
    \theta = \arccos{\left(\frac{\vec{x}\vec{y}}{\abs{\vec{x}}\abs{\vec{y}}}\right)}    
\]
since CS gives us that the fraction is less than or equal to $1$.

%%%%%%%%%%%%%%%%%%%%%%
% Lecture 4: 13/10/2023
%%%%%%%%%%%%%%%%%%%%%%

A further consequence of \eqref{eq:2-15} is the triangle inequality \eqref{eq:2}. We will consider the following
\begin{align*}
    \abs{\vec{x} + \vec{y}}^2 &= (\vec{x} + \vec{y}) \cdot (\vec{x} + \vec{y}) \\
    &= \vec{x} \cdot \vec{x} + 2 \vec{y} \cdot \vec{x} + \vec{y} \cdot \vec{y} \tag{collect terms over the reals} \\
    &= \abs{\vec{x}}^2 + \abs{\vec{y}}^2 + 2 \vec{y} \cdot \vec{x} \\
    &\leq \abs{\vec{x}}^2 + \abs{\vec{y}}^2 + 2 \abs{\vec{y} \cdot \vec{x}} \\
    &\leq \abs{\vec{x}}^2 + \abs{\vec{y}^2 + 2\abs{\vec{x}}}\abs{\vec{y}} \tag{By CS}\\
    &= (\abs{\vec{x}} + \abs{\vec{y}})^2 
\end{align*}
We may then take the positive square root on both sides to arive at
\begin{equation}
    \abs{\vec{x} + \vec{y}} \leq \abs{\vec{x}} + \abs{\vec{y}}
\end{equation}

\subsection{Vector Product}
We will see later in the course that this extends to the wedge product in higher dimensions. For now, we will define the product in $\R^3$.
\begin{defi}[Vector Product]
    Consider $\vec{a}, \vec{b} \in \R^3$. Define the vector product
    \[
        \vec{a} \times \vec{b} = \abs{\vec{a}}\abs{\vec{b}}\sin{\theta}\uvec{n}
    \]
    Where the direction of $\uvec{n}$ is such that $\vec{a}$, $\vec{b}$ and $\uvec{n}$ for a \emph{right handed system}. That is, when I curl my right hand fingers in the direction of $\theta$, the thumb points in the direction of $\uvec{n}$.
\end{defi}

Here are some properties of the vector product
\begin{enumerate}
    \item $\vec{a} \times \vec{b} - = \vec{b} \times \vec{a}$ \hspace*{\fill}(Anti-symmetric)
    \item $\vec{a} \times \vec{a} = \vec{0}$
    \item If $\vec{a} \times \vec{b} = \vec{0} \Rightarrow \vec{a} = \mu\vec{b}, \quad \text{for} \mu \in \R$
    \item $\vec{a} \times (\lambda \vec{b}) = \lambda (\vec{a} \times \vec{b})$
    \item[] \hspace*{\fill}\ilnumberthis
\end{enumerate}

\begin{defi}[Vector Area]
    We define the \emph{vector area} between two vectors $a$ and $b$ to be
    \[  
        \text{Vector Area} \equiv \frac{1}{2}(\vec{a} \times \vec{b})
    \]

    We may consider as an example the regular area of  a triangle, which can be found
    \begin{align*}
        \text{A} &= \frac{1}{2}\abs{\pvec{OB}}\abs{\pvec{BN}} \\
        &=  \frac{1}{2}\abs{\pvec{OB}}\abs{\pvec{OB}}\sin{\theta} \\
        &= \frac{1}{2}\abs{\vec{a} \times \vec{b}}
    \end{align*}
    
    Note that to turn this into a vector area, we omit the absolute value and use the vector itself to represent the area. An interesting consequence of this is that although the surface area of a sphere is $4\pi r^2$ the it's vector area is $\vec{0}$!
\end{defi}

\subsection{Scalar Triple Product}

\begin{defi}[Scalar Triple Product]
    Given three vectors $\vec{a}, \vec{b}, \vec{c} \in \R^3$, the triple product is defined as
    \begin{equation}
        \vec{a} \cdot (\vec{b} \times \vec{c}) = (\vec{b} \times \vec{c}) \cdot a = -a \cdot (c \times b)
    \end{equation}
    clearly, this is a scalar. Geometrically, we can interprit the triple product as the volume of a parallelopiped formed by $a$, $b$ and $c$, where the three vectors form a right handed system.
    
    % insert diagram of parallelopiped
    \begin{align*}
        \text{Volume} \ &= \ \text{Base Area} \ \times \perp \text{height} \\
        &\Rightarrow \abs{\vec{b} \times \vec{c}} \abs{\vec{a}} \sin{\theta} \\
        &= \abs{\vec{b} \times \vec{a}}\abs{\vec{a}}\cos{\bkt{\frac{\pi}{2} - \theta}} \\
        &= \abs{(\vec{b} \times \vec{a}) \cdot \vec{a}}
    \end{align*}
    Furthermore, since we specified that the vectors form a right handed system,
    \[
        \vec{a} \cdot (\vec{b} \times \vec{c}) \geq 0
    \]
\end{defi}

\begin{notation}
    We may instead write the tripple product using square brackets as follows
    \begin{equation}
        [\vec{a}, \vec{b}, \vec{c}] = [\vec{b}, \vec{c}, \vec{a}] =  [\vec{c}, \vec{a}, \vec{b}]
    \end{equation}

    where all even permutations are the equal, (even permutation meaning permutations performing an even number of pariwise swaps)
    \begin{equation}
        [\vec{a}, \vec{b}, \vec{c}] = - [\vec{b}, \vec{a}, \vec{c}] = - [\vec{a}, \vec{c}, \vec{b}] = - [\vec{c}, \vec{b}, \vec{a}]
    \end{equation}
    and all odd permutations are negative of the triple product.
\end{notation}

\begin{remark}
    The triple product can tell us if the three vectors are coplanar. Specifically,
    \begin{equation}
        \vec{a}, \vec{b}, \vec{c} \ \text{coplanar} \Leftrightarrow [\vec{a}, \vec{b}, \vec{c}] = 0  
    \end{equation}
\end{remark}

\begin{cor}
    Any triple product where two of the vectors are the same is equal to $0$.
    \[
        [\vec{a}, \vec{a}, \vec{c}] = [\vec{a}, \vec{b}, \vec{b}] = \ldots = 0  
    \]
\end{cor}

\subsection{Spanning sets \& Basis}
We will build up from the smaller example of $\R^2$, to $\R^3$, before working in $\R^n$. In general, spanning sets together with linear independance gives us a basis.

\begin{defi}[Spanning sets]
    Consider $2$ vectors $\vec{a}, \vec{b} \neq 0 \in \R^2$ with $\vec{a} \times \vec{b} \neq 0$ i.e; $\vec{a}$ and $\vec{b}$ are not parallel. Then, any other vector $\vec{r}$ in this plane can be written as 
    \begin{equation}
        \vec{r} = \lambda \vec{a} + \mu \vec{b}, \quad \text{for} \lambda, \mu \in \R \label{eq:2-22}
    \end{equation}
    Then we say that the set $\{\vec{a}, \vec{b}\}$ spans $\R^2$, and every other vector in the space can be written as a linear combination of $\vec{a}$ and $\vec{b}$.
\end{defi}

\begin{prop}
    \eqref{eq:2-22} produces a unique representation.
\end{prop}

\begin{proof}
    Suppose 
    \[
        r = \lambda \vec{a} + \mu \vec{b} = \lambda' \vec{a} + \mu' \vec{b}    
    \]
    a useful trick to employ it to apply cross product with one of the terms, allowing us to remove it. Taking $\times \vec{b}$ to both sides of the equation we get
    \[
        (\lambda - \lambda')\vec{a} \times \vec{b} = \vec{0}  
    \]
    finally, as we have defined $\vec{a}$ and $\vec{b}$ to be non parallel,
    \[
        \therefore \lambda - \lambda' = 0
    \]

    Similarly, we can multiply $\times \vec{a}$ to arrive at
    \[
        \mu - \mu' = 0  
    \]
    Hence \eqref{eq:2-22} is unique.
\end{proof}

\begin{defi}[Linear Independance]
    Two vectors $\vec{a}, \vec{b}$ are said to be \emph{linearly independant} if, $\forall \alpha, \beta \in \R$
    \begin{equation}
        \alpha \vec{a} + \beta \vec{b} = 0 \Rightarrow \alpha = \beta = 0 \label{eq:2-23}
    \end{equation}
    Later on, we will see that this is generalised into $\R^n$
\end{defi}

\begin{remark}
    $\vec{a}$ and $\vec{b}$ are linearly dependant in $\R^2$ if $\vec{a} \times \vec{b} \neq 0$. To see that it is the case, take
    \begin{align*}
        \vec{b} \times \eqref{eq:2-23} &\Rightarrow \alpha (\vec{b} \times \vec{a}) = 0 \\
        \vec{a} \times \eqref{eq:2-23} &\Rightarrow \beta (\vec{a} \times \vec{b}) = 0 \\
    \end{align*}
    In both cases, since $\vec{a} \times \vec{b} \neq 0$, $\alpha = \beta = 0$
\end{remark}

\begin{defi}[Basis]
    A set $\{\vec{a}, \vec{b}\}$ is a basis of $\R^n$ if it spans $\R^2$ and it is linearly independant.
\end{defi}

\begin{remark}
    Note that the number of dimensions of a space in fact follows from the size of the set of minimal basis vectors.
\end{remark}

\begin{eg}
    \begin{enumerate}
        \item $\{\uvec{i}, \uvec{j}\}$ is a basis of $\R^2$
        \item $\{\uvec{i}, \uvec{j}, \vec{e}\}$ where $\vec{e} = \uvec{i} + \uvec{j}$ certainly spans $\R^2$, but it does not form a basis since the three vector system is not linearly independant. To see take,
        \[
            \uvec{i} + \uvec{j} + \vec{e} = \vec{0}    
        \]
    \end{enumerate}
\end{eg}

\begin{prop}
    A basis is the smallest spanning set
\end{prop}

\begin{proof}
    See Example sheet exercise
\end{proof}

\begin{defi}[Basis in $\R^3$]
    If $\vec{a}, \vec{b}, \vec{c}$ are non-coplanar, then $\{\vec{a}, \vec{b}, \vec{c}\}$ is a basis of $\R^3$. We can write any vector $r$ as 
    \begin{equation}
        \vec{r} = \lambda \vec{a} + \mu \vec{b} + \nu \vec{c}, \quad \text{for}, \lambda, \mu, \nu \in \R \label{eq:2-24}
    \end{equation}
\end{defi}
\begin{proof}
    To show that it is uniquely defined, we will proceed by construction of the variables. Using a similar trick, using the tripple product to get rid of terms as repeated vectors in the triple product go to zero. $\eqref{eq:2-24} \cdot (\vec{b} \times \vec{c})$
    \begin{align*}
        \vec{r} \cdot (\vec{b} \times \vec{c}) &= \lambda \vec{a} \cdot (\vec{b} \times \vec{c}) \\
        &\Rightarrow \lambda = \frac{\vec{r} \cdot (\vec{b} \times \vec{c})}{\vec{a} \cdot (\vec{b} \times \vec{c})} = \frac{\vec{r} \cdot (\vec{b} \times \vec{c})}{[\vec{a}, \vec{b}, \vec{c}]}\\
        \vec{r} \cdot (\vec{c} \times \vec{a}) &= \mu \vec{b} \cdot (\vec{c} \times \vec{a}) \\
        &\Rightarrow \mu = \frac{\vec{r} \cdot (\vec{c} \times \vec{a})}{\vec{b} \cdot (\vec{c} \times \vec{a})}  = \frac{\vec{r} \cdot (\vec{c} \times \vec{a})}{[\vec{a}, \vec{b}, \vec{c}]} \\
        \vec{r} \cdot (\vec{a} \times \vec{b}) &= \nu \vec{c} \cdot (\vec{a} \times \vec{b}) \\
        &\Rightarrow \nu = \frac{\vec{r} \cdot (\vec{a} \times \vec{b})}{\vec{c} \cdot (\vec{a} \times \vec{b})}  = \frac{\vec{r} \cdot (\vec{a} \times \vec{b})}{[\vec{a}, \vec{b}, \vec{c}]} \\
    \end{align*}
    Hence, $\lambda, \mu, \nu$ can be determined for any $\vec{r}$ and $\{\vec{a}, \vec{b}, \vec{c}\}$ spans $\R^3$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%
% Lecture 5: _/10/2023
%%%%%%%%%%%%%%%%%%%%%%
\end{document}